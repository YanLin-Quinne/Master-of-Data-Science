{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9d7ddb",
   "metadata": {
    "id": "d5b95bca"
   },
   "source": [
    "## Machine Learning Courseworkâ€”Neural network part\n",
    "\n",
    "### Written by Yan Lin,  submitted on 20 February, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33652a5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33652a5d",
    "outputId": "9e8bbab4-2756-429d-fea5-fd4af9c69d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee5d67c2",
   "metadata": {
    "id": "ee5d67c2"
   },
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "column_names = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides',\n",
    "                'free sulfur dioxide', 'total sulfur dioxide', 'density','pH','sulphates','alcohol','quality']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, na_values='?', comment='\\t',sep=';', skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fa0fcd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9fa0fcd4",
    "outputId": "7dc8b050-ed71-409a-9891-e566dcba8e03"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f8369",
   "metadata": {
    "id": "0c4f8369"
   },
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48c80e55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48c80e55",
    "outputId": "9cfd33ce-e4f4-4ca4-ea83-b9fd74da330d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0015cad8",
   "metadata": {
    "id": "0015cad8"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5967631f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5967631f",
    "outputId": "227ad532-b288-4490-d809-951584dfa513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed acidity           float64\n",
      "volatile acidity        float64\n",
      "citric acid             float64\n",
      "residual sugar          float64\n",
      "chlorides               float64\n",
      "free sulfur dioxide     float64\n",
      "total sulfur dioxide    float64\n",
      "density                 float64\n",
      "pH                      float64\n",
      "sulphates               float64\n",
      "alcohol                 float64\n",
      "quality                   int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "print(dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f68d056",
   "metadata": {
    "id": "8f68d056"
   },
   "outputs": [],
   "source": [
    "# Convert 'quality' variable to binary class\n",
    "dataset['quality'] = dataset['quality'].apply(lambda x: 1 if x >= 7 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74f3ff91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022697</td>\n",
       "      <td>0.289181</td>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.023086</td>\n",
       "      <td>-0.049396</td>\n",
       "      <td>0.091070</td>\n",
       "      <td>0.265331</td>\n",
       "      <td>-0.425858</td>\n",
       "      <td>-0.017143</td>\n",
       "      <td>-0.120881</td>\n",
       "      <td>-0.080748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>-0.022697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.149472</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.070512</td>\n",
       "      <td>-0.097012</td>\n",
       "      <td>0.089261</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>-0.031915</td>\n",
       "      <td>-0.035728</td>\n",
       "      <td>0.067718</td>\n",
       "      <td>-0.067225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>0.289181</td>\n",
       "      <td>-0.149472</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094212</td>\n",
       "      <td>0.114364</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>0.121131</td>\n",
       "      <td>0.149503</td>\n",
       "      <td>-0.163748</td>\n",
       "      <td>0.062331</td>\n",
       "      <td>-0.075729</td>\n",
       "      <td>-0.035330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.094212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.088685</td>\n",
       "      <td>0.299098</td>\n",
       "      <td>0.401439</td>\n",
       "      <td>0.838966</td>\n",
       "      <td>-0.194133</td>\n",
       "      <td>-0.026664</td>\n",
       "      <td>-0.450631</td>\n",
       "      <td>-0.117085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>0.023086</td>\n",
       "      <td>0.070512</td>\n",
       "      <td>0.114364</td>\n",
       "      <td>0.088685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101392</td>\n",
       "      <td>0.198910</td>\n",
       "      <td>0.257211</td>\n",
       "      <td>-0.090439</td>\n",
       "      <td>0.016763</td>\n",
       "      <td>-0.360189</td>\n",
       "      <td>-0.183118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>-0.049396</td>\n",
       "      <td>-0.097012</td>\n",
       "      <td>0.094077</td>\n",
       "      <td>0.299098</td>\n",
       "      <td>0.101392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.615501</td>\n",
       "      <td>0.294210</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.059217</td>\n",
       "      <td>-0.250104</td>\n",
       "      <td>-0.023413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>0.091070</td>\n",
       "      <td>0.089261</td>\n",
       "      <td>0.121131</td>\n",
       "      <td>0.401439</td>\n",
       "      <td>0.198910</td>\n",
       "      <td>0.615501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.529881</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>-0.448892</td>\n",
       "      <td>-0.162202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>0.265331</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>0.149503</td>\n",
       "      <td>0.838966</td>\n",
       "      <td>0.257211</td>\n",
       "      <td>0.294210</td>\n",
       "      <td>0.529881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.093591</td>\n",
       "      <td>0.074493</td>\n",
       "      <td>-0.780138</td>\n",
       "      <td>-0.283871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>-0.425858</td>\n",
       "      <td>-0.031915</td>\n",
       "      <td>-0.163748</td>\n",
       "      <td>-0.194133</td>\n",
       "      <td>-0.090439</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>-0.093591</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>0.121432</td>\n",
       "      <td>0.093510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>-0.017143</td>\n",
       "      <td>-0.035728</td>\n",
       "      <td>0.062331</td>\n",
       "      <td>-0.026664</td>\n",
       "      <td>0.016763</td>\n",
       "      <td>0.059217</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>0.074493</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017433</td>\n",
       "      <td>0.047410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>-0.120881</td>\n",
       "      <td>0.067718</td>\n",
       "      <td>-0.075729</td>\n",
       "      <td>-0.450631</td>\n",
       "      <td>-0.360189</td>\n",
       "      <td>-0.250104</td>\n",
       "      <td>-0.448892</td>\n",
       "      <td>-0.780138</td>\n",
       "      <td>0.121432</td>\n",
       "      <td>-0.017433</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.385132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.080748</td>\n",
       "      <td>-0.067225</td>\n",
       "      <td>-0.035330</td>\n",
       "      <td>-0.117085</td>\n",
       "      <td>-0.183118</td>\n",
       "      <td>-0.023413</td>\n",
       "      <td>-0.162202</td>\n",
       "      <td>-0.283871</td>\n",
       "      <td>0.093510</td>\n",
       "      <td>0.047410</td>\n",
       "      <td>0.385132</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      fixed acidity  volatile acidity  citric acid  \\\n",
       "fixed acidity              1.000000         -0.022697     0.289181   \n",
       "volatile acidity          -0.022697          1.000000    -0.149472   \n",
       "citric acid                0.289181         -0.149472     1.000000   \n",
       "residual sugar             0.089021          0.064286     0.094212   \n",
       "chlorides                  0.023086          0.070512     0.114364   \n",
       "free sulfur dioxide       -0.049396         -0.097012     0.094077   \n",
       "total sulfur dioxide       0.091070          0.089261     0.121131   \n",
       "density                    0.265331          0.027114     0.149503   \n",
       "pH                        -0.425858         -0.031915    -0.163748   \n",
       "sulphates                 -0.017143         -0.035728     0.062331   \n",
       "alcohol                   -0.120881          0.067718    -0.075729   \n",
       "quality                   -0.080748         -0.067225    -0.035330   \n",
       "\n",
       "                      residual sugar  chlorides  free sulfur dioxide  \\\n",
       "fixed acidity               0.089021   0.023086            -0.049396   \n",
       "volatile acidity            0.064286   0.070512            -0.097012   \n",
       "citric acid                 0.094212   0.114364             0.094077   \n",
       "residual sugar              1.000000   0.088685             0.299098   \n",
       "chlorides                   0.088685   1.000000             0.101392   \n",
       "free sulfur dioxide         0.299098   0.101392             1.000000   \n",
       "total sulfur dioxide        0.401439   0.198910             0.615501   \n",
       "density                     0.838966   0.257211             0.294210   \n",
       "pH                         -0.194133  -0.090439            -0.000618   \n",
       "sulphates                  -0.026664   0.016763             0.059217   \n",
       "alcohol                    -0.450631  -0.360189            -0.250104   \n",
       "quality                    -0.117085  -0.183118            -0.023413   \n",
       "\n",
       "                      total sulfur dioxide   density        pH  sulphates  \\\n",
       "fixed acidity                     0.091070  0.265331 -0.425858  -0.017143   \n",
       "volatile acidity                  0.089261  0.027114 -0.031915  -0.035728   \n",
       "citric acid                       0.121131  0.149503 -0.163748   0.062331   \n",
       "residual sugar                    0.401439  0.838966 -0.194133  -0.026664   \n",
       "chlorides                         0.198910  0.257211 -0.090439   0.016763   \n",
       "free sulfur dioxide               0.615501  0.294210 -0.000618   0.059217   \n",
       "total sulfur dioxide              1.000000  0.529881  0.002321   0.134562   \n",
       "density                           0.529881  1.000000 -0.093591   0.074493   \n",
       "pH                                0.002321 -0.093591  1.000000   0.155951   \n",
       "sulphates                         0.134562  0.074493  0.155951   1.000000   \n",
       "alcohol                          -0.448892 -0.780138  0.121432  -0.017433   \n",
       "quality                          -0.162202 -0.283871  0.093510   0.047410   \n",
       "\n",
       "                       alcohol   quality  \n",
       "fixed acidity        -0.120881 -0.080748  \n",
       "volatile acidity      0.067718 -0.067225  \n",
       "citric acid          -0.075729 -0.035330  \n",
       "residual sugar       -0.450631 -0.117085  \n",
       "chlorides            -0.360189 -0.183118  \n",
       "free sulfur dioxide  -0.250104 -0.023413  \n",
       "total sulfur dioxide -0.448892 -0.162202  \n",
       "density              -0.780138 -0.283871  \n",
       "pH                    0.121432  0.093510  \n",
       "sulphates            -0.017433  0.047410  \n",
       "alcohol               1.000000  0.385132  \n",
       "quality               0.385132  1.000000  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the correlation coefficient matrix\n",
    "cor_matrix = dataset.corr()\n",
    "cor_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73abd09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data set into two subsets\n",
    "d2_0 = dataset[dataset.quality == 0].iloc[:, :-1]\n",
    "d2_1 = dataset[dataset.quality == 1].iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96605d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance matrix and covariance matrix for each subset\n",
    "var_matrix_0 = np.var(d2_0, axis=0)\n",
    "var_matrix_1 = np.var(d2_1, axis=0)\n",
    "\n",
    "cov_matrix_0 = np.cov(d2_0.T)\n",
    "cov_matrix_1 = np.cov(d2_1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "236aab09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fixed acidity              0.739594\n",
       " volatile acidity           0.010461\n",
       " citric acid                0.016885\n",
       " residual sugar            27.294014\n",
       " chlorides                  0.000554\n",
       " free sulfur dioxide      316.324283\n",
       " total sulfur dioxide    1948.308789\n",
       " density                    0.000008\n",
       " pH                         0.022021\n",
       " sulphates                  0.011697\n",
       " alcohol                    1.210951\n",
       " dtype: float64,\n",
       " fixed acidity              0.590493\n",
       " volatile acidity           0.008846\n",
       " citric acid                0.006440\n",
       " residual sugar            18.393985\n",
       " chlorides                  0.000124\n",
       " free sulfur dioxide      190.181651\n",
       " total sulfur dioxide    1069.905402\n",
       " density                    0.000008\n",
       " pH                         0.024684\n",
       " sulphates                  0.017684\n",
       " alcohol                    1.574064\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_matrix_0,var_matrix_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e4b64a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>0.739786</td>\n",
       "      <td>-0.001064</td>\n",
       "      <td>0.032992</td>\n",
       "      <td>0.207067</td>\n",
       "      <td>-0.000205</td>\n",
       "      <td>-0.943361</td>\n",
       "      <td>2.253866</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>-0.051838</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>-0.037553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>-0.001064</td>\n",
       "      <td>0.010464</td>\n",
       "      <td>-0.001873</td>\n",
       "      <td>0.041570</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>-0.155567</td>\n",
       "      <td>0.513420</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.000707</td>\n",
       "      <td>-0.000305</td>\n",
       "      <td>-0.001738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>0.032992</td>\n",
       "      <td>-0.001873</td>\n",
       "      <td>0.016889</td>\n",
       "      <td>0.066537</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.206014</td>\n",
       "      <td>0.680726</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.003441</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>-0.008349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>0.207067</td>\n",
       "      <td>0.041570</td>\n",
       "      <td>0.066537</td>\n",
       "      <td>27.301127</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>30.673526</td>\n",
       "      <td>87.988364</td>\n",
       "      <td>0.012907</td>\n",
       "      <td>-0.113968</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>-2.511217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>-0.000205</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.038477</td>\n",
       "      <td>0.161859</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.007840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>-0.943361</td>\n",
       "      <td>-0.155567</td>\n",
       "      <td>0.206014</td>\n",
       "      <td>30.673526</td>\n",
       "      <td>0.038477</td>\n",
       "      <td>316.406723</td>\n",
       "      <td>496.111385</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>-0.006801</td>\n",
       "      <td>0.067361</td>\n",
       "      <td>-5.540173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>2.253866</td>\n",
       "      <td>0.513420</td>\n",
       "      <td>0.680726</td>\n",
       "      <td>87.988364</td>\n",
       "      <td>0.161859</td>\n",
       "      <td>496.111385</td>\n",
       "      <td>1948.816558</td>\n",
       "      <td>0.064413</td>\n",
       "      <td>0.194902</td>\n",
       "      <td>0.871520</td>\n",
       "      <td>-20.693733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.012907</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>0.064413</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.002341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>-0.051838</td>\n",
       "      <td>-0.000707</td>\n",
       "      <td>-0.003441</td>\n",
       "      <td>-0.113968</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>-0.006801</td>\n",
       "      <td>0.194902</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.022027</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.013291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>0.001115</td>\n",
       "      <td>-0.000305</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.067361</td>\n",
       "      <td>0.871520</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>-0.004385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>-0.037553</td>\n",
       "      <td>-0.001738</td>\n",
       "      <td>-0.008349</td>\n",
       "      <td>-2.511217</td>\n",
       "      <td>-0.007840</td>\n",
       "      <td>-5.540173</td>\n",
       "      <td>-20.693733</td>\n",
       "      <td>-0.002341</td>\n",
       "      <td>0.013291</td>\n",
       "      <td>-0.004385</td>\n",
       "      <td>1.211267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      fixed acidity  volatile acidity  citric acid  \\\n",
       "fixed acidity              0.739786         -0.001064     0.032992   \n",
       "volatile acidity          -0.001064          0.010464    -0.001873   \n",
       "citric acid                0.032992         -0.001873     0.016889   \n",
       "residual sugar             0.207067          0.041570     0.066537   \n",
       "chlorides                 -0.000205          0.000247     0.000338   \n",
       "free sulfur dioxide       -0.943361         -0.155567     0.206014   \n",
       "total sulfur dioxide       2.253866          0.513420     0.680726   \n",
       "density                    0.000524          0.000024     0.000057   \n",
       "pH                        -0.051838         -0.000707    -0.003441   \n",
       "sulphates                  0.001115         -0.000305     0.001205   \n",
       "alcohol                   -0.037553         -0.001738    -0.008349   \n",
       "\n",
       "                      residual sugar  chlorides  free sulfur dioxide  \\\n",
       "fixed acidity               0.207067  -0.000205            -0.943361   \n",
       "volatile acidity            0.041570   0.000247            -0.155567   \n",
       "citric acid                 0.066537   0.000338             0.206014   \n",
       "residual sugar             27.301127   0.005776            30.673526   \n",
       "chlorides                   0.005776   0.000554             0.038477   \n",
       "free sulfur dioxide        30.673526   0.038477           316.406723   \n",
       "total sulfur dioxide       87.988364   0.161859           496.111385   \n",
       "density                     0.012907   0.000013             0.016813   \n",
       "pH                         -0.113968  -0.000254            -0.006801   \n",
       "sulphates                   0.005439   0.000068             0.067361   \n",
       "alcohol                    -2.511217  -0.007840            -5.540173   \n",
       "\n",
       "                      total sulfur dioxide   density        pH  sulphates  \\\n",
       "fixed acidity                     2.253866  0.000524 -0.051838   0.001115   \n",
       "volatile acidity                  0.513420  0.000024 -0.000707  -0.000305   \n",
       "citric acid                       0.680726  0.000057 -0.003441   0.001205   \n",
       "residual sugar                   87.988364  0.012907 -0.113968   0.005439   \n",
       "chlorides                         0.161859  0.000013 -0.000254   0.000068   \n",
       "free sulfur dioxide             496.111385  0.016813 -0.006801   0.067361   \n",
       "total sulfur dioxide           1948.816558  0.064413  0.194902   0.871520   \n",
       "density                           0.064413  0.000008 -0.000016   0.000036   \n",
       "pH                                0.194902 -0.000016  0.022027   0.002171   \n",
       "sulphates                         0.871520  0.000036  0.002171   0.011700   \n",
       "alcohol                         -20.693733 -0.002341  0.013291  -0.004385   \n",
       "\n",
       "                        alcohol  \n",
       "fixed acidity         -0.037553  \n",
       "volatile acidity      -0.001738  \n",
       "citric acid           -0.008349  \n",
       "residual sugar        -2.511217  \n",
       "chlorides             -0.007840  \n",
       "free sulfur dioxide   -5.540173  \n",
       "total sulfur dioxide -20.693733  \n",
       "density               -0.002341  \n",
       "pH                     0.013291  \n",
       "sulphates             -0.004385  \n",
       "alcohol                1.211267  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_df_0=pd.DataFrame(cov_matrix_0, columns=d2_0.columns, index=d2_0.columns)\n",
    "cov_df_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ca8d946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>0.591050</td>\n",
       "      <td>-0.007206</td>\n",
       "      <td>0.015680</td>\n",
       "      <td>0.824538</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.014381</td>\n",
       "      <td>4.764318</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>-0.058659</td>\n",
       "      <td>-0.009971</td>\n",
       "      <td>-0.295082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>-0.007206</td>\n",
       "      <td>0.008854</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>-0.217825</td>\n",
       "      <td>-0.308155</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>-0.000625</td>\n",
       "      <td>0.059990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>0.015680</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.141068</td>\n",
       "      <td>0.278093</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.012533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>0.824538</td>\n",
       "      <td>-0.017252</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>18.411355</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>7.075646</td>\n",
       "      <td>62.403926</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>-0.235830</td>\n",
       "      <td>-0.076221</td>\n",
       "      <td>-2.606353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>0.001451</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.013530</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.027436</td>\n",
       "      <td>0.140047</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.007606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>0.014381</td>\n",
       "      <td>-0.217825</td>\n",
       "      <td>0.141068</td>\n",
       "      <td>7.075646</td>\n",
       "      <td>0.027436</td>\n",
       "      <td>190.361237</td>\n",
       "      <td>246.919148</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.043305</td>\n",
       "      <td>0.297391</td>\n",
       "      <td>-3.259269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>4.764318</td>\n",
       "      <td>-0.308155</td>\n",
       "      <td>0.278093</td>\n",
       "      <td>62.403926</td>\n",
       "      <td>0.140047</td>\n",
       "      <td>246.919148</td>\n",
       "      <td>1070.915700</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>-0.187214</td>\n",
       "      <td>0.032694</td>\n",
       "      <td>-18.474242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>0.000932</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.002935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>-0.058659</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>-0.235830</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>0.043305</td>\n",
       "      <td>-0.187214</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>0.024707</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.025242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>-0.009971</td>\n",
       "      <td>-0.000625</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.076221</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.297391</td>\n",
       "      <td>0.032694</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.017701</td>\n",
       "      <td>-0.007292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>-0.295082</td>\n",
       "      <td>0.059990</td>\n",
       "      <td>-0.012533</td>\n",
       "      <td>-2.606353</td>\n",
       "      <td>-0.007606</td>\n",
       "      <td>-3.259269</td>\n",
       "      <td>-18.474242</td>\n",
       "      <td>-0.002935</td>\n",
       "      <td>0.025242</td>\n",
       "      <td>-0.007292</td>\n",
       "      <td>1.575551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      fixed acidity  volatile acidity  citric acid  \\\n",
       "fixed acidity              0.591050         -0.007206     0.015680   \n",
       "volatile acidity          -0.007206          0.008854    -0.001780   \n",
       "citric acid                0.015680         -0.001780     0.006446   \n",
       "residual sugar             0.824538         -0.017252     0.014592   \n",
       "chlorides                  0.001451         -0.000301     0.000094   \n",
       "free sulfur dioxide        0.014381         -0.217825     0.141068   \n",
       "total sulfur dioxide       4.764318         -0.308155     0.278093   \n",
       "density                    0.000932         -0.000077     0.000029   \n",
       "pH                        -0.058659          0.000759    -0.001091   \n",
       "sulphates                 -0.009971         -0.000625    -0.000278   \n",
       "alcohol                   -0.295082          0.059990    -0.012533   \n",
       "\n",
       "                      residual sugar  chlorides  free sulfur dioxide  \\\n",
       "fixed acidity               0.824538   0.001451             0.014381   \n",
       "volatile acidity           -0.017252  -0.000301            -0.217825   \n",
       "citric acid                 0.014592   0.000094             0.141068   \n",
       "residual sugar             18.411355   0.013530             7.075646   \n",
       "chlorides                   0.013530   0.000124             0.027436   \n",
       "free sulfur dioxide         7.075646   0.027436           190.361237   \n",
       "total sulfur dioxide       62.403926   0.140047           246.919148   \n",
       "density                     0.009755   0.000015             0.006724   \n",
       "pH                         -0.235830  -0.000199             0.043305   \n",
       "sulphates                  -0.076221   0.000047             0.297391   \n",
       "alcohol                    -2.606353  -0.007606            -3.259269   \n",
       "\n",
       "                      total sulfur dioxide   density        pH  sulphates  \\\n",
       "fixed acidity                     4.764318  0.000932 -0.058659  -0.009971   \n",
       "volatile acidity                 -0.308155 -0.000077  0.000759  -0.000625   \n",
       "citric acid                       0.278093  0.000029 -0.001091  -0.000278   \n",
       "residual sugar                   62.403926  0.009755 -0.235830  -0.076221   \n",
       "chlorides                         0.140047  0.000015 -0.000199   0.000047   \n",
       "free sulfur dioxide             246.919148  0.006724  0.043305   0.297391   \n",
       "total sulfur dioxide           1070.915700  0.051000 -0.187214   0.032694   \n",
       "density                           0.051000  0.000008 -0.000082   0.000007   \n",
       "pH                               -0.187214 -0.000082  0.024707   0.004209   \n",
       "sulphates                         0.032694  0.000007  0.004209   0.017701   \n",
       "alcohol                         -18.474242 -0.002935  0.025242  -0.007292   \n",
       "\n",
       "                        alcohol  \n",
       "fixed acidity         -0.295082  \n",
       "volatile acidity       0.059990  \n",
       "citric acid           -0.012533  \n",
       "residual sugar        -2.606353  \n",
       "chlorides             -0.007606  \n",
       "free sulfur dioxide   -3.259269  \n",
       "total sulfur dioxide -18.474242  \n",
       "density               -0.002935  \n",
       "pH                     0.025242  \n",
       "sulphates             -0.007292  \n",
       "alcohol                1.575551  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_df_1=pd.DataFrame(cov_matrix_1, columns=d2_0.columns, index=d2_0.columns)\n",
    "cov_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3624ee96",
   "metadata": {
    "id": "3624ee96"
   },
   "outputs": [],
   "source": [
    "# Dividing the training and test sets\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9811f505",
   "metadata": {
    "id": "9811f505"
   },
   "outputs": [],
   "source": [
    "# Distinguishing features and labels\n",
    "train_features = train_dataset.drop('quality', axis=1)\n",
    "test_features = test_dataset.drop('quality', axis=1)\n",
    "\n",
    "train_labels = train_dataset['quality']\n",
    "test_labels = test_dataset['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a87ab",
   "metadata": {
    "id": "e54a87ab"
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "034783d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "034783d4",
    "outputId": "5dd1ccbf-d1aa-4207-883b-e578c355aa9d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fixed acidity</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>6.854788</td>\n",
       "      <td>0.843868</td>\n",
       "      <td>3.80000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>6.80000</td>\n",
       "      <td>7.3000</td>\n",
       "      <td>14.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatile acidity</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>0.278241</td>\n",
       "      <td>0.100795</td>\n",
       "      <td>0.08000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>1.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citric acid</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>0.334192</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.32000</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>1.66000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residual sugar</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>6.391415</td>\n",
       "      <td>5.072058</td>\n",
       "      <td>0.60000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>9.9000</td>\n",
       "      <td>65.80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chlorides</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>0.045772</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.04300</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.34600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>35.308085</td>\n",
       "      <td>17.007137</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>46.0000</td>\n",
       "      <td>289.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>138.360657</td>\n",
       "      <td>42.498065</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>167.0000</td>\n",
       "      <td>440.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>0.994027</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.98711</td>\n",
       "      <td>0.991723</td>\n",
       "      <td>0.99374</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>1.03898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pH</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>3.188267</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>2.72000</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>3.18000</td>\n",
       "      <td>3.2800</td>\n",
       "      <td>3.82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sulphates</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>0.489847</td>\n",
       "      <td>0.114126</td>\n",
       "      <td>0.22000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.47000</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>1.08000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>10.514267</td>\n",
       "      <td>1.230621</td>\n",
       "      <td>8.00000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>10.40000</td>\n",
       "      <td>11.4000</td>\n",
       "      <td>14.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>4898.0</td>\n",
       "      <td>0.216415</td>\n",
       "      <td>0.411842</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count        mean        std      min         25%  \\\n",
       "fixed acidity         4898.0    6.854788   0.843868  3.80000    6.300000   \n",
       "volatile acidity      4898.0    0.278241   0.100795  0.08000    0.210000   \n",
       "citric acid           4898.0    0.334192   0.121020  0.00000    0.270000   \n",
       "residual sugar        4898.0    6.391415   5.072058  0.60000    1.700000   \n",
       "chlorides             4898.0    0.045772   0.021848  0.00900    0.036000   \n",
       "free sulfur dioxide   4898.0   35.308085  17.007137  2.00000   23.000000   \n",
       "total sulfur dioxide  4898.0  138.360657  42.498065  9.00000  108.000000   \n",
       "density               4898.0    0.994027   0.002991  0.98711    0.991723   \n",
       "pH                    4898.0    3.188267   0.151001  2.72000    3.090000   \n",
       "sulphates             4898.0    0.489847   0.114126  0.22000    0.410000   \n",
       "alcohol               4898.0   10.514267   1.230621  8.00000    9.500000   \n",
       "quality               4898.0    0.216415   0.411842  0.00000    0.000000   \n",
       "\n",
       "                            50%       75%        max  \n",
       "fixed acidity           6.80000    7.3000   14.20000  \n",
       "volatile acidity        0.26000    0.3200    1.10000  \n",
       "citric acid             0.32000    0.3900    1.66000  \n",
       "residual sugar          5.20000    9.9000   65.80000  \n",
       "chlorides               0.04300    0.0500    0.34600  \n",
       "free sulfur dioxide    34.00000   46.0000  289.00000  \n",
       "total sulfur dioxide  134.00000  167.0000  440.00000  \n",
       "density                 0.99374    0.9961    1.03898  \n",
       "pH                      3.18000    3.2800    3.82000  \n",
       "sulphates               0.47000    0.5500    1.08000  \n",
       "alcohol                10.40000   11.4000   14.20000  \n",
       "quality                 0.00000    0.0000    1.00000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7ab14d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7ab14d3",
    "outputId": "79a31065-24b7-442e-da6b-c20bf5d05d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "256b1805",
   "metadata": {
    "id": "256b1805"
   },
   "outputs": [],
   "source": [
    "datavalues=train_features.values\n",
    "data_train_features_norm=np.copy(datavalues)\n",
    "data_train_features_norm[:,0:11]=(datavalues[:,0:11]-np.mean(datavalues[:,0:11],axis=0))/np.std(datavalues[:,0:11],axis=0)\n",
    "datavalues=test_features.values\n",
    "data_test_features_norm=np.copy(datavalues)\n",
    "data_test_features_norm[:,0:11]=(datavalues[:,0:11]-np.mean(datavalues[:,0:11],axis=0))/np.std(datavalues[:,0:11],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ae22ece",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ae22ece",
    "outputId": "8f8b97e1-b453-4b98-b2bb-40f8eca22145"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.516,  0.417,  0.125, ...,  0.348, -0.605,  0.149],\n",
       "       [ 0.165,  0.317, -0.612, ..., -0.38 , -0.255, -0.583],\n",
       "       [ 0.867, -1.381,  3.318, ..., -0.777, -0.781,  0.23 ],\n",
       "       ...,\n",
       "       [-0.069, -0.682, -0.53 , ...,  0.745, -1.131, -1.477],\n",
       "       [ 0.399,  1.215, -0.775, ...,  0.083,  0.271, -1.07 ],\n",
       "       [-0.419,  1.615,  1.271, ..., -0.512, -1.043, -1.477]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_features_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4ec3089",
   "metadata": {
    "id": "e4ec3089"
   },
   "outputs": [],
   "source": [
    "alcohol_train_norm = data_train_features_norm[:,10]\n",
    "alcohol_test_norm = data_test_features_norm[:,10]\n",
    "train_label_values=train_labels.values\n",
    "test_label_values=test_labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "EladvovnXeDg",
   "metadata": {
    "id": "EladvovnXeDg"
   },
   "outputs": [],
   "source": [
    "#Classification problems with dnn models\n",
    "num_classes=2\n",
    "batch_size = 128\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "j3rbNwImXeGh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3rbNwImXeGh",
    "outputId": "ba15980d-1927-4ab7-8c26-3324d23d9767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 11)                0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 32)                384       \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,618\n",
      "Trainable params: 3,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model1:four hidden layer+softmax\n",
    "model_DNN_1 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=11),   \n",
    "        layers.Flatten(),   \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_DNN_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1440c5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5561 - accuracy: 0.7513 - val_loss: 0.4855 - val_accuracy: 0.7934\n",
      "Epoch 2/250\n",
      "28/28 [==============================] - 0s 987us/step - loss: 0.4702 - accuracy: 0.7805 - val_loss: 0.4451 - val_accuracy: 0.7934\n",
      "Epoch 3/250\n",
      "28/28 [==============================] - 0s 958us/step - loss: 0.4263 - accuracy: 0.7873 - val_loss: 0.4236 - val_accuracy: 0.7883\n",
      "Epoch 4/250\n",
      "28/28 [==============================] - 0s 904us/step - loss: 0.3997 - accuracy: 0.7986 - val_loss: 0.4107 - val_accuracy: 0.7755\n",
      "Epoch 5/250\n",
      "28/28 [==============================] - 0s 988us/step - loss: 0.3880 - accuracy: 0.8037 - val_loss: 0.4090 - val_accuracy: 0.7985\n",
      "Epoch 6/250\n",
      "28/28 [==============================] - 0s 899us/step - loss: 0.3797 - accuracy: 0.8080 - val_loss: 0.3993 - val_accuracy: 0.7908\n",
      "Epoch 7/250\n",
      "28/28 [==============================] - 0s 922us/step - loss: 0.3724 - accuracy: 0.8094 - val_loss: 0.3979 - val_accuracy: 0.8010\n",
      "Epoch 8/250\n",
      "28/28 [==============================] - 0s 930us/step - loss: 0.3688 - accuracy: 0.8134 - val_loss: 0.3939 - val_accuracy: 0.8087\n",
      "Epoch 9/250\n",
      "28/28 [==============================] - 0s 890us/step - loss: 0.3621 - accuracy: 0.8157 - val_loss: 0.3864 - val_accuracy: 0.8061\n",
      "Epoch 10/250\n",
      "28/28 [==============================] - 0s 948us/step - loss: 0.3578 - accuracy: 0.8157 - val_loss: 0.3838 - val_accuracy: 0.8138\n",
      "Epoch 11/250\n",
      "28/28 [==============================] - 0s 937us/step - loss: 0.3539 - accuracy: 0.8216 - val_loss: 0.3845 - val_accuracy: 0.8087\n",
      "Epoch 12/250\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.3491 - accuracy: 0.8279 - val_loss: 0.3850 - val_accuracy: 0.8087\n",
      "Epoch 13/250\n",
      "28/28 [==============================] - 0s 895us/step - loss: 0.3457 - accuracy: 0.8281 - val_loss: 0.3769 - val_accuracy: 0.8189\n",
      "Epoch 14/250\n",
      "28/28 [==============================] - 0s 933us/step - loss: 0.3404 - accuracy: 0.8330 - val_loss: 0.3759 - val_accuracy: 0.8240\n",
      "Epoch 15/250\n",
      "28/28 [==============================] - 0s 943us/step - loss: 0.3370 - accuracy: 0.8375 - val_loss: 0.3759 - val_accuracy: 0.8265\n",
      "Epoch 16/250\n",
      "28/28 [==============================] - 0s 899us/step - loss: 0.3322 - accuracy: 0.8417 - val_loss: 0.3704 - val_accuracy: 0.8240\n",
      "Epoch 17/250\n",
      "28/28 [==============================] - 0s 944us/step - loss: 0.3306 - accuracy: 0.8452 - val_loss: 0.3695 - val_accuracy: 0.8138\n",
      "Epoch 18/250\n",
      "28/28 [==============================] - 0s 914us/step - loss: 0.3257 - accuracy: 0.8454 - val_loss: 0.3746 - val_accuracy: 0.8061\n",
      "Epoch 19/250\n",
      "28/28 [==============================] - 0s 938us/step - loss: 0.3218 - accuracy: 0.8477 - val_loss: 0.3679 - val_accuracy: 0.8036\n",
      "Epoch 20/250\n",
      "28/28 [==============================] - 0s 952us/step - loss: 0.3190 - accuracy: 0.8417 - val_loss: 0.3665 - val_accuracy: 0.8112\n",
      "Epoch 21/250\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.3144 - accuracy: 0.8486 - val_loss: 0.3685 - val_accuracy: 0.8061\n",
      "Epoch 22/250\n",
      "28/28 [==============================] - 0s 952us/step - loss: 0.3123 - accuracy: 0.8514 - val_loss: 0.3637 - val_accuracy: 0.8138\n",
      "Epoch 23/250\n",
      "28/28 [==============================] - 0s 966us/step - loss: 0.3061 - accuracy: 0.8542 - val_loss: 0.3618 - val_accuracy: 0.8087\n",
      "Epoch 24/250\n",
      "28/28 [==============================] - 0s 940us/step - loss: 0.2998 - accuracy: 0.8582 - val_loss: 0.3622 - val_accuracy: 0.8214\n",
      "Epoch 25/250\n",
      "28/28 [==============================] - 0s 984us/step - loss: 0.2975 - accuracy: 0.8590 - val_loss: 0.3653 - val_accuracy: 0.8061\n",
      "Epoch 26/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2940 - accuracy: 0.8613 - val_loss: 0.3631 - val_accuracy: 0.8112\n",
      "Epoch 27/250\n",
      "28/28 [==============================] - 0s 884us/step - loss: 0.2874 - accuracy: 0.8627 - val_loss: 0.3653 - val_accuracy: 0.8138\n",
      "Epoch 28/250\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.2847 - accuracy: 0.8616 - val_loss: 0.3728 - val_accuracy: 0.8087\n",
      "Epoch 29/250\n",
      "28/28 [==============================] - 0s 915us/step - loss: 0.2812 - accuracy: 0.8684 - val_loss: 0.3541 - val_accuracy: 0.8214\n",
      "Epoch 30/250\n",
      "28/28 [==============================] - 0s 892us/step - loss: 0.2761 - accuracy: 0.8698 - val_loss: 0.3666 - val_accuracy: 0.8214\n",
      "Epoch 31/250\n",
      "28/28 [==============================] - 0s 972us/step - loss: 0.2753 - accuracy: 0.8715 - val_loss: 0.3583 - val_accuracy: 0.8214\n",
      "Epoch 32/250\n",
      "28/28 [==============================] - 0s 894us/step - loss: 0.2715 - accuracy: 0.8729 - val_loss: 0.3633 - val_accuracy: 0.8342\n",
      "Epoch 33/250\n",
      "28/28 [==============================] - 0s 912us/step - loss: 0.2674 - accuracy: 0.8744 - val_loss: 0.3804 - val_accuracy: 0.8010\n",
      "Epoch 34/250\n",
      "28/28 [==============================] - 0s 891us/step - loss: 0.2699 - accuracy: 0.8738 - val_loss: 0.3540 - val_accuracy: 0.8367\n",
      "Epoch 35/250\n",
      "28/28 [==============================] - 0s 911us/step - loss: 0.2568 - accuracy: 0.8840 - val_loss: 0.3586 - val_accuracy: 0.8367\n",
      "Epoch 36/250\n",
      "28/28 [==============================] - 0s 951us/step - loss: 0.2540 - accuracy: 0.8885 - val_loss: 0.3737 - val_accuracy: 0.8189\n",
      "Epoch 37/250\n",
      "28/28 [==============================] - 0s 867us/step - loss: 0.2516 - accuracy: 0.8815 - val_loss: 0.3593 - val_accuracy: 0.8342\n",
      "Epoch 38/250\n",
      "28/28 [==============================] - 0s 894us/step - loss: 0.2484 - accuracy: 0.8871 - val_loss: 0.3720 - val_accuracy: 0.8418\n",
      "Epoch 39/250\n",
      "28/28 [==============================] - 0s 864us/step - loss: 0.2467 - accuracy: 0.8897 - val_loss: 0.3668 - val_accuracy: 0.8367\n",
      "Epoch 40/250\n",
      "28/28 [==============================] - 0s 906us/step - loss: 0.2387 - accuracy: 0.8945 - val_loss: 0.3732 - val_accuracy: 0.8495\n",
      "Epoch 41/250\n",
      "28/28 [==============================] - 0s 865us/step - loss: 0.2370 - accuracy: 0.8936 - val_loss: 0.3745 - val_accuracy: 0.8444\n",
      "Epoch 42/250\n",
      "28/28 [==============================] - 0s 901us/step - loss: 0.2358 - accuracy: 0.8951 - val_loss: 0.3803 - val_accuracy: 0.8367\n",
      "Epoch 43/250\n",
      "28/28 [==============================] - 0s 868us/step - loss: 0.2276 - accuracy: 0.8999 - val_loss: 0.3767 - val_accuracy: 0.8571\n",
      "Epoch 44/250\n",
      "28/28 [==============================] - 0s 917us/step - loss: 0.2313 - accuracy: 0.8985 - val_loss: 0.3847 - val_accuracy: 0.8520\n",
      "Epoch 45/250\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.2273 - accuracy: 0.9007 - val_loss: 0.3798 - val_accuracy: 0.8418\n",
      "Epoch 46/250\n",
      "28/28 [==============================] - 0s 862us/step - loss: 0.2255 - accuracy: 0.9022 - val_loss: 0.3817 - val_accuracy: 0.8495\n",
      "Epoch 47/250\n",
      "28/28 [==============================] - 0s 906us/step - loss: 0.2188 - accuracy: 0.9027 - val_loss: 0.3861 - val_accuracy: 0.8520\n",
      "Epoch 48/250\n",
      "28/28 [==============================] - 0s 901us/step - loss: 0.2140 - accuracy: 0.9098 - val_loss: 0.3909 - val_accuracy: 0.8546\n",
      "Epoch 49/250\n",
      "28/28 [==============================] - 0s 985us/step - loss: 0.2115 - accuracy: 0.9067 - val_loss: 0.4032 - val_accuracy: 0.8418\n",
      "Epoch 50/250\n",
      "28/28 [==============================] - 0s 949us/step - loss: 0.2125 - accuracy: 0.9075 - val_loss: 0.4389 - val_accuracy: 0.7985\n",
      "Epoch 51/250\n",
      "28/28 [==============================] - 0s 959us/step - loss: 0.2092 - accuracy: 0.9138 - val_loss: 0.4366 - val_accuracy: 0.8061\n",
      "Epoch 52/250\n",
      "28/28 [==============================] - 0s 975us/step - loss: 0.2051 - accuracy: 0.9163 - val_loss: 0.3997 - val_accuracy: 0.8495\n",
      "Epoch 53/250\n",
      "28/28 [==============================] - 0s 972us/step - loss: 0.1984 - accuracy: 0.9209 - val_loss: 0.4087 - val_accuracy: 0.8571\n",
      "Epoch 54/250\n",
      "28/28 [==============================] - 0s 902us/step - loss: 0.1963 - accuracy: 0.9226 - val_loss: 0.4329 - val_accuracy: 0.8265\n",
      "Epoch 55/250\n",
      "28/28 [==============================] - 0s 971us/step - loss: 0.1904 - accuracy: 0.9240 - val_loss: 0.4101 - val_accuracy: 0.8597\n",
      "Epoch 56/250\n",
      "28/28 [==============================] - 0s 980us/step - loss: 0.1899 - accuracy: 0.9240 - val_loss: 0.4137 - val_accuracy: 0.8597\n",
      "Epoch 57/250\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.1881 - accuracy: 0.9260 - val_loss: 0.4181 - val_accuracy: 0.8546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9254 - val_loss: 0.4193 - val_accuracy: 0.8520\n",
      "Epoch 59/250\n",
      "28/28 [==============================] - 0s 941us/step - loss: 0.1822 - accuracy: 0.9285 - val_loss: 0.4481 - val_accuracy: 0.8444\n",
      "Epoch 60/250\n",
      "28/28 [==============================] - 0s 931us/step - loss: 0.1796 - accuracy: 0.9274 - val_loss: 0.4485 - val_accuracy: 0.8469\n",
      "Epoch 61/250\n",
      "28/28 [==============================] - 0s 971us/step - loss: 0.1756 - accuracy: 0.9291 - val_loss: 0.4416 - val_accuracy: 0.8495\n",
      "Epoch 62/250\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.1762 - accuracy: 0.9314 - val_loss: 0.4617 - val_accuracy: 0.8393\n",
      "Epoch 63/250\n",
      "28/28 [==============================] - 0s 902us/step - loss: 0.1746 - accuracy: 0.9311 - val_loss: 0.4767 - val_accuracy: 0.8342\n",
      "Epoch 64/250\n",
      "28/28 [==============================] - 0s 924us/step - loss: 0.1801 - accuracy: 0.9248 - val_loss: 0.4405 - val_accuracy: 0.8571\n",
      "Epoch 65/250\n",
      "28/28 [==============================] - 0s 879us/step - loss: 0.1696 - accuracy: 0.9334 - val_loss: 0.4711 - val_accuracy: 0.8367\n",
      "Epoch 66/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.1690 - accuracy: 0.9345 - val_loss: 0.4551 - val_accuracy: 0.8546\n",
      "Epoch 67/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.1694 - accuracy: 0.9331 - val_loss: 0.4825 - val_accuracy: 0.8418\n",
      "Epoch 68/250\n",
      "28/28 [==============================] - 0s 942us/step - loss: 0.1717 - accuracy: 0.9285 - val_loss: 0.5047 - val_accuracy: 0.8342\n",
      "Epoch 69/250\n",
      "28/28 [==============================] - 0s 958us/step - loss: 0.1631 - accuracy: 0.9362 - val_loss: 0.4640 - val_accuracy: 0.8571\n",
      "Epoch 70/250\n",
      "28/28 [==============================] - 0s 896us/step - loss: 0.1573 - accuracy: 0.9396 - val_loss: 0.4845 - val_accuracy: 0.8393\n",
      "Epoch 71/250\n",
      "28/28 [==============================] - 0s 899us/step - loss: 0.1524 - accuracy: 0.9399 - val_loss: 0.4850 - val_accuracy: 0.8571\n",
      "Epoch 72/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.1508 - accuracy: 0.9433 - val_loss: 0.4843 - val_accuracy: 0.8597\n",
      "Epoch 73/250\n",
      "28/28 [==============================] - 0s 902us/step - loss: 0.1506 - accuracy: 0.9416 - val_loss: 0.5366 - val_accuracy: 0.8291\n",
      "Epoch 74/250\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.1603 - accuracy: 0.9356 - val_loss: 0.4907 - val_accuracy: 0.8622\n",
      "Epoch 75/250\n",
      "28/28 [==============================] - 0s 970us/step - loss: 0.1465 - accuracy: 0.9433 - val_loss: 0.5098 - val_accuracy: 0.8418\n",
      "Epoch 76/250\n",
      "28/28 [==============================] - 0s 958us/step - loss: 0.1457 - accuracy: 0.9424 - val_loss: 0.5338 - val_accuracy: 0.8112\n",
      "Epoch 77/250\n",
      "28/28 [==============================] - 0s 862us/step - loss: 0.1447 - accuracy: 0.9441 - val_loss: 0.5251 - val_accuracy: 0.8342\n",
      "Epoch 78/250\n",
      "28/28 [==============================] - 0s 905us/step - loss: 0.1456 - accuracy: 0.9402 - val_loss: 0.5202 - val_accuracy: 0.8571\n",
      "Epoch 79/250\n",
      "28/28 [==============================] - 0s 884us/step - loss: 0.1404 - accuracy: 0.9436 - val_loss: 0.5108 - val_accuracy: 0.8571\n",
      "Epoch 80/250\n",
      "28/28 [==============================] - 0s 953us/step - loss: 0.1371 - accuracy: 0.9478 - val_loss: 0.5192 - val_accuracy: 0.8444\n",
      "Epoch 81/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9518 - val_loss: 0.5300 - val_accuracy: 0.8495\n",
      "Epoch 82/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9447 - val_loss: 0.5128 - val_accuracy: 0.8495\n",
      "Epoch 83/250\n",
      "28/28 [==============================] - 0s 919us/step - loss: 0.1317 - accuracy: 0.9487 - val_loss: 0.5436 - val_accuracy: 0.8622\n",
      "Epoch 84/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9501 - val_loss: 0.5435 - val_accuracy: 0.8520\n",
      "Epoch 85/250\n",
      "28/28 [==============================] - 0s 892us/step - loss: 0.1276 - accuracy: 0.9484 - val_loss: 0.5327 - val_accuracy: 0.8469\n",
      "Epoch 86/250\n",
      "28/28 [==============================] - 0s 878us/step - loss: 0.1270 - accuracy: 0.9504 - val_loss: 0.5392 - val_accuracy: 0.8342\n",
      "Epoch 87/250\n",
      "28/28 [==============================] - 0s 857us/step - loss: 0.1228 - accuracy: 0.9558 - val_loss: 0.5395 - val_accuracy: 0.8622\n",
      "Epoch 88/250\n",
      "28/28 [==============================] - 0s 852us/step - loss: 0.1298 - accuracy: 0.9504 - val_loss: 0.5669 - val_accuracy: 0.8316\n",
      "Epoch 89/250\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.1326 - accuracy: 0.9453 - val_loss: 0.5414 - val_accuracy: 0.8469\n",
      "Epoch 90/250\n",
      "28/28 [==============================] - 0s 880us/step - loss: 0.1174 - accuracy: 0.9552 - val_loss: 0.5461 - val_accuracy: 0.8571\n",
      "Epoch 91/250\n",
      "28/28 [==============================] - 0s 947us/step - loss: 0.1144 - accuracy: 0.9589 - val_loss: 0.5612 - val_accuracy: 0.8597\n",
      "Epoch 92/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9518 - val_loss: 0.5566 - val_accuracy: 0.8520\n",
      "Epoch 93/250\n",
      "28/28 [==============================] - 0s 915us/step - loss: 0.1212 - accuracy: 0.9526 - val_loss: 0.5745 - val_accuracy: 0.8520\n",
      "Epoch 94/250\n",
      "28/28 [==============================] - 0s 997us/step - loss: 0.1163 - accuracy: 0.9575 - val_loss: 0.5975 - val_accuracy: 0.8418\n",
      "Epoch 95/250\n",
      "28/28 [==============================] - 0s 998us/step - loss: 0.1133 - accuracy: 0.9549 - val_loss: 0.6450 - val_accuracy: 0.8112\n",
      "Epoch 96/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9549 - val_loss: 0.5938 - val_accuracy: 0.8469\n",
      "Epoch 97/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9626 - val_loss: 0.6054 - val_accuracy: 0.8520\n",
      "Epoch 98/250\n",
      "28/28 [==============================] - 0s 952us/step - loss: 0.1044 - accuracy: 0.9654 - val_loss: 0.6112 - val_accuracy: 0.8393\n",
      "Epoch 99/250\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1020 - accuracy: 0.9648 - val_loss: 0.6036 - val_accuracy: 0.8495\n",
      "Epoch 100/250\n",
      "28/28 [==============================] - 0s 954us/step - loss: 0.1064 - accuracy: 0.9589 - val_loss: 0.6171 - val_accuracy: 0.8469\n",
      "Epoch 101/250\n",
      "28/28 [==============================] - 0s 928us/step - loss: 0.1023 - accuracy: 0.9614 - val_loss: 0.6227 - val_accuracy: 0.8444\n",
      "Epoch 102/250\n",
      "28/28 [==============================] - 0s 951us/step - loss: 0.1001 - accuracy: 0.9626 - val_loss: 0.6500 - val_accuracy: 0.8444\n",
      "Epoch 103/250\n",
      "28/28 [==============================] - 0s 919us/step - loss: 0.1019 - accuracy: 0.9631 - val_loss: 0.6849 - val_accuracy: 0.8316\n",
      "Epoch 104/250\n",
      "28/28 [==============================] - 0s 922us/step - loss: 0.1033 - accuracy: 0.9586 - val_loss: 0.6521 - val_accuracy: 0.8418\n",
      "Epoch 105/250\n",
      "28/28 [==============================] - 0s 953us/step - loss: 0.0956 - accuracy: 0.9654 - val_loss: 0.6777 - val_accuracy: 0.8393\n",
      "Epoch 106/250\n",
      "28/28 [==============================] - 0s 959us/step - loss: 0.0933 - accuracy: 0.9668 - val_loss: 0.6547 - val_accuracy: 0.8444\n",
      "Epoch 107/250\n",
      "28/28 [==============================] - 0s 915us/step - loss: 0.0997 - accuracy: 0.9648 - val_loss: 0.6989 - val_accuracy: 0.8495\n",
      "Epoch 108/250\n",
      "28/28 [==============================] - 0s 944us/step - loss: 0.0918 - accuracy: 0.9651 - val_loss: 0.6958 - val_accuracy: 0.8469\n",
      "Epoch 109/250\n",
      "28/28 [==============================] - 0s 994us/step - loss: 0.0935 - accuracy: 0.9668 - val_loss: 0.7154 - val_accuracy: 0.8316\n",
      "Epoch 110/250\n",
      "28/28 [==============================] - 0s 915us/step - loss: 0.0889 - accuracy: 0.9671 - val_loss: 0.7397 - val_accuracy: 0.8342\n",
      "Epoch 111/250\n",
      "28/28 [==============================] - 0s 960us/step - loss: 0.0892 - accuracy: 0.9674 - val_loss: 0.6824 - val_accuracy: 0.8469\n",
      "Epoch 112/250\n",
      "28/28 [==============================] - 0s 967us/step - loss: 0.0944 - accuracy: 0.9648 - val_loss: 0.6858 - val_accuracy: 0.8367\n",
      "Epoch 113/250\n",
      "28/28 [==============================] - 0s 955us/step - loss: 0.0834 - accuracy: 0.9702 - val_loss: 0.7090 - val_accuracy: 0.8418\n",
      "Epoch 114/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.0937 - accuracy: 0.9628 - val_loss: 0.7056 - val_accuracy: 0.8444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/250\n",
      "28/28 [==============================] - 0s 928us/step - loss: 0.0986 - accuracy: 0.9583 - val_loss: 0.7195 - val_accuracy: 0.8342\n",
      "Epoch 116/250\n",
      "28/28 [==============================] - 0s 897us/step - loss: 0.0908 - accuracy: 0.9648 - val_loss: 0.7189 - val_accuracy: 0.8393\n",
      "Epoch 117/250\n",
      "28/28 [==============================] - 0s 933us/step - loss: 0.0883 - accuracy: 0.9665 - val_loss: 0.7015 - val_accuracy: 0.8316\n",
      "Epoch 118/250\n",
      "28/28 [==============================] - 0s 875us/step - loss: 0.0879 - accuracy: 0.9660 - val_loss: 0.7181 - val_accuracy: 0.8444\n",
      "Epoch 119/250\n",
      "28/28 [==============================] - 0s 918us/step - loss: 0.0759 - accuracy: 0.9736 - val_loss: 0.7567 - val_accuracy: 0.8418\n",
      "Epoch 120/250\n",
      "28/28 [==============================] - 0s 897us/step - loss: 0.0839 - accuracy: 0.9708 - val_loss: 0.7603 - val_accuracy: 0.8393\n",
      "Epoch 121/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.0741 - accuracy: 0.9762 - val_loss: 0.7841 - val_accuracy: 0.8367\n",
      "Epoch 122/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9722 - val_loss: 0.7679 - val_accuracy: 0.8520\n",
      "Epoch 123/250\n",
      "28/28 [==============================] - 0s 890us/step - loss: 0.0736 - accuracy: 0.9736 - val_loss: 0.7652 - val_accuracy: 0.8342\n",
      "Epoch 124/250\n",
      "28/28 [==============================] - 0s 912us/step - loss: 0.0864 - accuracy: 0.9682 - val_loss: 0.7696 - val_accuracy: 0.8469\n",
      "Epoch 125/250\n",
      "28/28 [==============================] - 0s 950us/step - loss: 0.0803 - accuracy: 0.9716 - val_loss: 0.8008 - val_accuracy: 0.8393\n",
      "Epoch 126/250\n",
      "28/28 [==============================] - 0s 910us/step - loss: 0.0721 - accuracy: 0.9765 - val_loss: 0.8099 - val_accuracy: 0.8240\n",
      "Epoch 127/250\n",
      "28/28 [==============================] - 0s 936us/step - loss: 0.0683 - accuracy: 0.9762 - val_loss: 0.8115 - val_accuracy: 0.8342\n",
      "Epoch 128/250\n",
      "28/28 [==============================] - 0s 921us/step - loss: 0.0709 - accuracy: 0.9767 - val_loss: 0.7981 - val_accuracy: 0.8444\n",
      "Epoch 129/250\n",
      "28/28 [==============================] - 0s 930us/step - loss: 0.0655 - accuracy: 0.9801 - val_loss: 0.8248 - val_accuracy: 0.8342\n",
      "Epoch 130/250\n",
      "28/28 [==============================] - 0s 937us/step - loss: 0.0641 - accuracy: 0.9790 - val_loss: 0.8252 - val_accuracy: 0.8393\n",
      "Epoch 131/250\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.0682 - accuracy: 0.9779 - val_loss: 0.8278 - val_accuracy: 0.8393\n",
      "Epoch 132/250\n",
      "28/28 [==============================] - 0s 939us/step - loss: 0.0655 - accuracy: 0.9784 - val_loss: 0.8772 - val_accuracy: 0.8418\n",
      "Epoch 133/250\n",
      "28/28 [==============================] - 0s 959us/step - loss: 0.0639 - accuracy: 0.9784 - val_loss: 0.8190 - val_accuracy: 0.8393\n",
      "Epoch 134/250\n",
      "28/28 [==============================] - 0s 913us/step - loss: 0.0662 - accuracy: 0.9773 - val_loss: 0.8586 - val_accuracy: 0.8444\n",
      "Epoch 135/250\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.0658 - accuracy: 0.9756 - val_loss: 0.9120 - val_accuracy: 0.8265\n",
      "Epoch 136/250\n",
      "28/28 [==============================] - 0s 898us/step - loss: 0.0674 - accuracy: 0.9762 - val_loss: 0.8784 - val_accuracy: 0.8393\n",
      "Epoch 137/250\n",
      "28/28 [==============================] - 0s 955us/step - loss: 0.0682 - accuracy: 0.9767 - val_loss: 0.8828 - val_accuracy: 0.8342\n",
      "Epoch 138/250\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.0618 - accuracy: 0.9776 - val_loss: 0.8836 - val_accuracy: 0.8393\n",
      "Epoch 139/250\n",
      "28/28 [==============================] - 0s 907us/step - loss: 0.0660 - accuracy: 0.9759 - val_loss: 0.9241 - val_accuracy: 0.8367\n",
      "Epoch 140/250\n",
      "28/28 [==============================] - 0s 937us/step - loss: 0.0745 - accuracy: 0.9728 - val_loss: 0.9077 - val_accuracy: 0.8393\n",
      "Epoch 141/250\n",
      "28/28 [==============================] - 0s 915us/step - loss: 0.0646 - accuracy: 0.9773 - val_loss: 0.8757 - val_accuracy: 0.8469\n",
      "Epoch 142/250\n",
      "28/28 [==============================] - 0s 950us/step - loss: 0.0618 - accuracy: 0.9796 - val_loss: 0.9437 - val_accuracy: 0.8367\n",
      "Epoch 143/250\n",
      "28/28 [==============================] - 0s 938us/step - loss: 0.0570 - accuracy: 0.9816 - val_loss: 0.8963 - val_accuracy: 0.8418\n",
      "Epoch 144/250\n",
      "28/28 [==============================] - 0s 898us/step - loss: 0.0527 - accuracy: 0.9858 - val_loss: 0.8816 - val_accuracy: 0.8444\n",
      "Epoch 145/250\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.0567 - accuracy: 0.9801 - val_loss: 0.9198 - val_accuracy: 0.8444\n",
      "Epoch 146/250\n",
      "28/28 [==============================] - 0s 918us/step - loss: 0.0508 - accuracy: 0.9841 - val_loss: 0.9315 - val_accuracy: 0.8444\n",
      "Epoch 147/250\n",
      "28/28 [==============================] - 0s 932us/step - loss: 0.0476 - accuracy: 0.9864 - val_loss: 0.9424 - val_accuracy: 0.8393\n",
      "Epoch 148/250\n",
      "28/28 [==============================] - 0s 944us/step - loss: 0.0490 - accuracy: 0.9841 - val_loss: 0.9901 - val_accuracy: 0.8393\n",
      "Epoch 149/250\n",
      "28/28 [==============================] - 0s 913us/step - loss: 0.0534 - accuracy: 0.9824 - val_loss: 0.9732 - val_accuracy: 0.8418\n",
      "Epoch 150/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.0535 - accuracy: 0.9830 - val_loss: 0.9863 - val_accuracy: 0.8393\n",
      "Epoch 151/250\n",
      "28/28 [==============================] - 0s 901us/step - loss: 0.0545 - accuracy: 0.9853 - val_loss: 0.9783 - val_accuracy: 0.8418\n",
      "Epoch 152/250\n",
      "28/28 [==============================] - 0s 944us/step - loss: 0.0507 - accuracy: 0.9844 - val_loss: 0.9861 - val_accuracy: 0.8469\n",
      "Epoch 153/250\n",
      "28/28 [==============================] - 0s 941us/step - loss: 0.0491 - accuracy: 0.9861 - val_loss: 0.9966 - val_accuracy: 0.8469\n",
      "Epoch 154/250\n",
      "28/28 [==============================] - 0s 917us/step - loss: 0.0524 - accuracy: 0.9827 - val_loss: 1.0024 - val_accuracy: 0.8418\n",
      "Epoch 155/250\n",
      "28/28 [==============================] - 0s 928us/step - loss: 0.0502 - accuracy: 0.9853 - val_loss: 1.0570 - val_accuracy: 0.8265\n",
      "Epoch 156/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0461 - accuracy: 0.9870 - val_loss: 1.0226 - val_accuracy: 0.8444\n",
      "Epoch 157/250\n",
      "28/28 [==============================] - 0s 966us/step - loss: 0.0475 - accuracy: 0.9850 - val_loss: 1.0772 - val_accuracy: 0.8367\n",
      "Epoch 158/250\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.0540 - accuracy: 0.9824 - val_loss: 1.0465 - val_accuracy: 0.8418\n",
      "Epoch 159/250\n",
      "28/28 [==============================] - 0s 947us/step - loss: 0.0443 - accuracy: 0.9858 - val_loss: 1.0728 - val_accuracy: 0.8469\n",
      "Epoch 160/250\n",
      "28/28 [==============================] - 0s 954us/step - loss: 0.0495 - accuracy: 0.9827 - val_loss: 1.0964 - val_accuracy: 0.8393\n",
      "Epoch 161/250\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.0497 - accuracy: 0.9816 - val_loss: 1.1420 - val_accuracy: 0.8265\n",
      "Epoch 162/250\n",
      "28/28 [==============================] - 0s 942us/step - loss: 0.0546 - accuracy: 0.9796 - val_loss: 1.0687 - val_accuracy: 0.8316\n",
      "Epoch 163/250\n",
      "28/28 [==============================] - 0s 954us/step - loss: 0.0467 - accuracy: 0.9861 - val_loss: 1.0893 - val_accuracy: 0.8418\n",
      "Epoch 164/250\n",
      "28/28 [==============================] - 0s 948us/step - loss: 0.0382 - accuracy: 0.9898 - val_loss: 1.0753 - val_accuracy: 0.8418\n",
      "Epoch 165/250\n",
      "28/28 [==============================] - 0s 955us/step - loss: 0.0357 - accuracy: 0.9904 - val_loss: 1.1234 - val_accuracy: 0.8444\n",
      "Epoch 166/250\n",
      "28/28 [==============================] - 0s 962us/step - loss: 0.0384 - accuracy: 0.9906 - val_loss: 1.1348 - val_accuracy: 0.8342\n",
      "Epoch 167/250\n",
      "28/28 [==============================] - 0s 936us/step - loss: 0.0403 - accuracy: 0.9895 - val_loss: 1.1464 - val_accuracy: 0.8291\n",
      "Epoch 168/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0387 - accuracy: 0.9889 - val_loss: 1.1501 - val_accuracy: 0.8265\n",
      "Epoch 169/250\n",
      "28/28 [==============================] - 0s 928us/step - loss: 0.0468 - accuracy: 0.9833 - val_loss: 1.1178 - val_accuracy: 0.8393\n",
      "Epoch 170/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0382 - accuracy: 0.9904 - val_loss: 1.1618 - val_accuracy: 0.8444\n",
      "Epoch 171/250\n",
      "28/28 [==============================] - 0s 981us/step - loss: 0.0363 - accuracy: 0.9901 - val_loss: 1.1782 - val_accuracy: 0.8316\n",
      "Epoch 172/250\n",
      "28/28 [==============================] - 0s 934us/step - loss: 0.0337 - accuracy: 0.9906 - val_loss: 1.1411 - val_accuracy: 0.8418\n",
      "Epoch 173/250\n",
      "28/28 [==============================] - 0s 943us/step - loss: 0.0338 - accuracy: 0.9904 - val_loss: 1.1644 - val_accuracy: 0.8444\n",
      "Epoch 174/250\n",
      "28/28 [==============================] - 0s 973us/step - loss: 0.0401 - accuracy: 0.9864 - val_loss: 1.1932 - val_accuracy: 0.8316\n",
      "Epoch 175/250\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.0586 - accuracy: 0.9776 - val_loss: 1.1203 - val_accuracy: 0.8546\n",
      "Epoch 176/250\n",
      "28/28 [==============================] - 0s 941us/step - loss: 0.0521 - accuracy: 0.9816 - val_loss: 1.1554 - val_accuracy: 0.8342\n",
      "Epoch 177/250\n",
      "28/28 [==============================] - 0s 964us/step - loss: 0.0634 - accuracy: 0.9841 - val_loss: 1.1840 - val_accuracy: 0.8291\n",
      "Epoch 178/250\n",
      "28/28 [==============================] - 0s 900us/step - loss: 0.0686 - accuracy: 0.9759 - val_loss: 1.1376 - val_accuracy: 0.8342\n",
      "Epoch 179/250\n",
      "28/28 [==============================] - 0s 932us/step - loss: 0.0447 - accuracy: 0.9838 - val_loss: 1.2119 - val_accuracy: 0.8418\n",
      "Epoch 180/250\n",
      "28/28 [==============================] - 0s 879us/step - loss: 0.0465 - accuracy: 0.9875 - val_loss: 1.1930 - val_accuracy: 0.8291\n",
      "Epoch 181/250\n",
      "28/28 [==============================] - 0s 930us/step - loss: 0.0434 - accuracy: 0.9853 - val_loss: 1.2160 - val_accuracy: 0.8418\n",
      "Epoch 182/250\n",
      "28/28 [==============================] - 0s 951us/step - loss: 0.0545 - accuracy: 0.9821 - val_loss: 1.2764 - val_accuracy: 0.8291\n",
      "Epoch 183/250\n",
      "28/28 [==============================] - 0s 916us/step - loss: 0.0463 - accuracy: 0.9833 - val_loss: 1.1767 - val_accuracy: 0.8342\n",
      "Epoch 184/250\n",
      "28/28 [==============================] - 0s 937us/step - loss: 0.0354 - accuracy: 0.9889 - val_loss: 1.1677 - val_accuracy: 0.8418\n",
      "Epoch 185/250\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.0381 - accuracy: 0.9898 - val_loss: 1.1921 - val_accuracy: 0.8342\n",
      "Epoch 186/250\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.0283 - accuracy: 0.9949 - val_loss: 1.2217 - val_accuracy: 0.8469\n",
      "Epoch 187/250\n",
      "28/28 [==============================] - 0s 928us/step - loss: 0.0256 - accuracy: 0.9935 - val_loss: 1.2502 - val_accuracy: 0.8393\n",
      "Epoch 188/250\n",
      "28/28 [==============================] - 0s 933us/step - loss: 0.0243 - accuracy: 0.9940 - val_loss: 1.2529 - val_accuracy: 0.8393\n",
      "Epoch 189/250\n",
      "28/28 [==============================] - 0s 917us/step - loss: 0.0236 - accuracy: 0.9943 - val_loss: 1.2490 - val_accuracy: 0.8393\n",
      "Epoch 190/250\n",
      "28/28 [==============================] - 0s 938us/step - loss: 0.0234 - accuracy: 0.9960 - val_loss: 1.2298 - val_accuracy: 0.8444\n",
      "Epoch 191/250\n",
      "28/28 [==============================] - 0s 911us/step - loss: 0.0230 - accuracy: 0.9957 - val_loss: 1.2578 - val_accuracy: 0.8316\n",
      "Epoch 192/250\n",
      "28/28 [==============================] - 0s 930us/step - loss: 0.0232 - accuracy: 0.9943 - val_loss: 1.2870 - val_accuracy: 0.8342\n",
      "Epoch 193/250\n",
      "28/28 [==============================] - 0s 935us/step - loss: 0.0229 - accuracy: 0.9949 - val_loss: 1.2612 - val_accuracy: 0.8367\n",
      "Epoch 194/250\n",
      "28/28 [==============================] - 0s 928us/step - loss: 0.0196 - accuracy: 0.9963 - val_loss: 1.2925 - val_accuracy: 0.8393\n",
      "Epoch 195/250\n",
      "28/28 [==============================] - 0s 979us/step - loss: 0.0187 - accuracy: 0.9966 - val_loss: 1.2936 - val_accuracy: 0.8342\n",
      "Epoch 196/250\n",
      "28/28 [==============================] - 0s 949us/step - loss: 0.0204 - accuracy: 0.9957 - val_loss: 1.2940 - val_accuracy: 0.8444\n",
      "Epoch 197/250\n",
      "28/28 [==============================] - 0s 916us/step - loss: 0.0206 - accuracy: 0.9960 - val_loss: 1.3011 - val_accuracy: 0.8444\n",
      "Epoch 198/250\n",
      "28/28 [==============================] - 0s 949us/step - loss: 0.0198 - accuracy: 0.9966 - val_loss: 1.3106 - val_accuracy: 0.8418\n",
      "Epoch 199/250\n",
      "28/28 [==============================] - 0s 977us/step - loss: 0.0255 - accuracy: 0.9952 - val_loss: 1.3253 - val_accuracy: 0.8393\n",
      "Epoch 200/250\n",
      "28/28 [==============================] - 0s 938us/step - loss: 0.0240 - accuracy: 0.9940 - val_loss: 1.3551 - val_accuracy: 0.8418\n",
      "Epoch 201/250\n",
      "28/28 [==============================] - 0s 998us/step - loss: 0.0219 - accuracy: 0.9946 - val_loss: 1.3299 - val_accuracy: 0.8418\n",
      "Epoch 202/250\n",
      "28/28 [==============================] - 0s 946us/step - loss: 0.0283 - accuracy: 0.9923 - val_loss: 1.3552 - val_accuracy: 0.8393\n",
      "Epoch 203/250\n",
      "28/28 [==============================] - 0s 938us/step - loss: 0.0345 - accuracy: 0.9864 - val_loss: 1.3235 - val_accuracy: 0.8393\n",
      "Epoch 204/250\n",
      "28/28 [==============================] - 0s 970us/step - loss: 0.0373 - accuracy: 0.9884 - val_loss: 1.2740 - val_accuracy: 0.8342\n",
      "Epoch 205/250\n",
      "28/28 [==============================] - 0s 970us/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 1.3565 - val_accuracy: 0.8367\n",
      "Epoch 206/250\n",
      "28/28 [==============================] - 0s 919us/step - loss: 0.0521 - accuracy: 0.9833 - val_loss: 1.3977 - val_accuracy: 0.8265\n",
      "Epoch 207/250\n",
      "28/28 [==============================] - 0s 940us/step - loss: 0.0431 - accuracy: 0.9847 - val_loss: 1.3834 - val_accuracy: 0.8367\n",
      "Epoch 208/250\n",
      "28/28 [==============================] - 0s 909us/step - loss: 0.0368 - accuracy: 0.9875 - val_loss: 1.3496 - val_accuracy: 0.8393\n",
      "Epoch 209/250\n",
      "28/28 [==============================] - 0s 984us/step - loss: 0.0464 - accuracy: 0.9858 - val_loss: 1.4653 - val_accuracy: 0.8112\n",
      "Epoch 210/250\n",
      "28/28 [==============================] - 0s 939us/step - loss: 0.0494 - accuracy: 0.9836 - val_loss: 1.3662 - val_accuracy: 0.8444\n",
      "Epoch 211/250\n",
      "28/28 [==============================] - 0s 938us/step - loss: 0.0365 - accuracy: 0.9884 - val_loss: 1.3342 - val_accuracy: 0.8469\n",
      "Epoch 212/250\n",
      "28/28 [==============================] - 0s 970us/step - loss: 0.0234 - accuracy: 0.9938 - val_loss: 1.3436 - val_accuracy: 0.8418\n",
      "Epoch 213/250\n",
      "28/28 [==============================] - 0s 925us/step - loss: 0.0196 - accuracy: 0.9960 - val_loss: 1.3661 - val_accuracy: 0.8495\n",
      "Epoch 214/250\n",
      "28/28 [==============================] - 0s 900us/step - loss: 0.0167 - accuracy: 0.9977 - val_loss: 1.3802 - val_accuracy: 0.8469\n",
      "Epoch 215/250\n",
      "28/28 [==============================] - 0s 943us/step - loss: 0.0141 - accuracy: 0.9983 - val_loss: 1.3831 - val_accuracy: 0.8444\n",
      "Epoch 216/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.0142 - accuracy: 0.9983 - val_loss: 1.3825 - val_accuracy: 0.8469\n",
      "Epoch 217/250\n",
      "28/28 [==============================] - 0s 947us/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 1.3962 - val_accuracy: 0.8444\n",
      "Epoch 218/250\n",
      "28/28 [==============================] - 0s 958us/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 1.4002 - val_accuracy: 0.8418\n",
      "Epoch 219/250\n",
      "28/28 [==============================] - 0s 928us/step - loss: 0.0124 - accuracy: 0.9989 - val_loss: 1.4243 - val_accuracy: 0.8469\n",
      "Epoch 220/250\n",
      "28/28 [==============================] - 0s 957us/step - loss: 0.0116 - accuracy: 0.9991 - val_loss: 1.4418 - val_accuracy: 0.8367\n",
      "Epoch 221/250\n",
      "28/28 [==============================] - 0s 936us/step - loss: 0.0119 - accuracy: 0.9991 - val_loss: 1.4399 - val_accuracy: 0.8444\n",
      "Epoch 222/250\n",
      "28/28 [==============================] - 0s 975us/step - loss: 0.0121 - accuracy: 0.9986 - val_loss: 1.4249 - val_accuracy: 0.8469\n",
      "Epoch 223/250\n",
      "28/28 [==============================] - 0s 951us/step - loss: 0.0120 - accuracy: 0.9983 - val_loss: 1.4390 - val_accuracy: 0.8444\n",
      "Epoch 224/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0108 - accuracy: 0.9991 - val_loss: 1.4735 - val_accuracy: 0.8469\n",
      "Epoch 225/250\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.0106 - accuracy: 0.9989 - val_loss: 1.4726 - val_accuracy: 0.8444\n",
      "Epoch 226/250\n",
      "28/28 [==============================] - 0s 976us/step - loss: 0.0114 - accuracy: 0.9989 - val_loss: 1.5235 - val_accuracy: 0.8393\n",
      "Epoch 227/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 984us/step - loss: 0.0101 - accuracy: 0.9991 - val_loss: 1.5130 - val_accuracy: 0.8444\n",
      "Epoch 228/250\n",
      "28/28 [==============================] - 0s 933us/step - loss: 0.0103 - accuracy: 0.9997 - val_loss: 1.5081 - val_accuracy: 0.8444\n",
      "Epoch 229/250\n",
      "28/28 [==============================] - 0s 947us/step - loss: 0.0099 - accuracy: 0.9994 - val_loss: 1.5069 - val_accuracy: 0.8444\n",
      "Epoch 230/250\n",
      "28/28 [==============================] - 0s 975us/step - loss: 0.0098 - accuracy: 0.9991 - val_loss: 1.5121 - val_accuracy: 0.8469\n",
      "Epoch 231/250\n",
      "28/28 [==============================] - 0s 915us/step - loss: 0.0113 - accuracy: 0.9991 - val_loss: 1.5293 - val_accuracy: 0.8444\n",
      "Epoch 232/250\n",
      "28/28 [==============================] - 0s 961us/step - loss: 0.0099 - accuracy: 0.9994 - val_loss: 1.5390 - val_accuracy: 0.8418\n",
      "Epoch 233/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0090 - accuracy: 0.9994 - val_loss: 1.5574 - val_accuracy: 0.8444\n",
      "Epoch 234/250\n",
      "28/28 [==============================] - 0s 930us/step - loss: 0.0116 - accuracy: 0.9980 - val_loss: 1.5460 - val_accuracy: 0.8418\n",
      "Epoch 235/250\n",
      "28/28 [==============================] - 0s 932us/step - loss: 0.0105 - accuracy: 0.9997 - val_loss: 1.5341 - val_accuracy: 0.8469\n",
      "Epoch 236/250\n",
      "28/28 [==============================] - 0s 948us/step - loss: 0.0105 - accuracy: 0.9989 - val_loss: 1.5745 - val_accuracy: 0.8444\n",
      "Epoch 237/250\n",
      "28/28 [==============================] - 0s 956us/step - loss: 0.0094 - accuracy: 0.9991 - val_loss: 1.5624 - val_accuracy: 0.8495\n",
      "Epoch 238/250\n",
      "28/28 [==============================] - 0s 907us/step - loss: 0.0126 - accuracy: 0.9977 - val_loss: 1.6057 - val_accuracy: 0.8393\n",
      "Epoch 239/250\n",
      "28/28 [==============================] - 0s 912us/step - loss: 0.0146 - accuracy: 0.9963 - val_loss: 1.5780 - val_accuracy: 0.8367\n",
      "Epoch 240/250\n",
      "28/28 [==============================] - 0s 960us/step - loss: 0.0172 - accuracy: 0.9963 - val_loss: 1.5889 - val_accuracy: 0.8418\n",
      "Epoch 241/250\n",
      "28/28 [==============================] - 0s 911us/step - loss: 0.0173 - accuracy: 0.9957 - val_loss: 1.6866 - val_accuracy: 0.8367\n",
      "Epoch 242/250\n",
      "28/28 [==============================] - 0s 940us/step - loss: 0.0325 - accuracy: 0.9921 - val_loss: 1.5787 - val_accuracy: 0.8469\n",
      "Epoch 243/250\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.1013 - accuracy: 0.9725 - val_loss: 1.5436 - val_accuracy: 0.8240\n",
      "Epoch 244/250\n",
      "28/28 [==============================] - 0s 912us/step - loss: 0.1515 - accuracy: 0.9594 - val_loss: 1.5000 - val_accuracy: 0.8418\n",
      "Epoch 245/250\n",
      "28/28 [==============================] - 0s 958us/step - loss: 0.1114 - accuracy: 0.9640 - val_loss: 1.5456 - val_accuracy: 0.8418\n",
      "Epoch 246/250\n",
      "28/28 [==============================] - 0s 939us/step - loss: 0.0637 - accuracy: 0.9799 - val_loss: 1.5310 - val_accuracy: 0.8291\n",
      "Epoch 247/250\n",
      "28/28 [==============================] - 0s 917us/step - loss: 0.0379 - accuracy: 0.9884 - val_loss: 1.5058 - val_accuracy: 0.8520\n",
      "Epoch 248/250\n",
      "28/28 [==============================] - 0s 971us/step - loss: 0.0264 - accuracy: 0.9921 - val_loss: 1.4606 - val_accuracy: 0.8393\n",
      "Epoch 249/250\n",
      "28/28 [==============================] - 0s 921us/step - loss: 0.0344 - accuracy: 0.9895 - val_loss: 1.5378 - val_accuracy: 0.8444\n",
      "Epoch 250/250\n",
      "28/28 [==============================] - 0s 946us/step - loss: 0.0241 - accuracy: 0.9929 - val_loss: 1.5280 - val_accuracy: 0.8444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291749880>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DNN_1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_DNN_1.fit(data_train_features_norm, keras.utils.to_categorical(train_label_values,num_classes), \n",
    "              batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "#Overfitting, the training and validation sets are very different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "SKFeozSoXeN0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKFeozSoXeN0",
    "outputId": "c3462f83-b0b2-4a81-bf2e-d9943badb38c"
   },
   "outputs": [],
   "source": [
    "# model_DNN.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# model_DNN.fit(data_train_features_norm, train_label_values, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "B_kpv6eMXeRM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B_kpv6eMXeRM",
    "outputId": "c4f3a598-e38b-416b-820d-e9dbaf881ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3344687223434448\n",
      "Test accuracy: 0.8479591608047485\n"
     ]
    }
   ],
   "source": [
    "score_1 = model_DNN_1.evaluate(data_test_features_norm, keras.utils.to_categorical(test_label_values, num_classes), verbose=0)\n",
    "print(\"Test loss:\", score_1[0])\n",
    "print(\"Test accuracy:\", score_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b291f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 11)                0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 32)                384       \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,618\n",
      "Trainable params: 3,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Four hidden layers + sigmoid\n",
    "model_DNN_2 = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=11),   \n",
    "        layers.Flatten(),   \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_DNN_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b15492d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "28/28 [==============================] - 1s 4ms/step - loss: 0.5109 - accuracy: 0.7805 - val_loss: 0.4677 - val_accuracy: 0.7934\n",
      "Epoch 2/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4546 - accuracy: 0.7930 - val_loss: 0.4438 - val_accuracy: 0.7959\n",
      "Epoch 3/250\n",
      "28/28 [==============================] - 0s 922us/step - loss: 0.4251 - accuracy: 0.8088 - val_loss: 0.4268 - val_accuracy: 0.7985\n",
      "Epoch 4/250\n",
      "28/28 [==============================] - 0s 919us/step - loss: 0.4060 - accuracy: 0.8094 - val_loss: 0.4215 - val_accuracy: 0.7985\n",
      "Epoch 5/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8128 - val_loss: 0.4160 - val_accuracy: 0.7832\n",
      "Epoch 6/250\n",
      "28/28 [==============================] - 0s 924us/step - loss: 0.3884 - accuracy: 0.8137 - val_loss: 0.4065 - val_accuracy: 0.8061\n",
      "Epoch 7/250\n",
      "28/28 [==============================] - 0s 959us/step - loss: 0.3793 - accuracy: 0.8185 - val_loss: 0.3978 - val_accuracy: 0.8087\n",
      "Epoch 8/250\n",
      "28/28 [==============================] - 0s 920us/step - loss: 0.3716 - accuracy: 0.8225 - val_loss: 0.3956 - val_accuracy: 0.8087\n",
      "Epoch 9/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.3651 - accuracy: 0.8244 - val_loss: 0.3959 - val_accuracy: 0.8189\n",
      "Epoch 10/250\n",
      "28/28 [==============================] - 0s 956us/step - loss: 0.3632 - accuracy: 0.8208 - val_loss: 0.3888 - val_accuracy: 0.8240\n",
      "Epoch 11/250\n",
      "28/28 [==============================] - 0s 905us/step - loss: 0.3576 - accuracy: 0.8261 - val_loss: 0.3878 - val_accuracy: 0.8214\n",
      "Epoch 12/250\n",
      "28/28 [==============================] - 0s 921us/step - loss: 0.3537 - accuracy: 0.8261 - val_loss: 0.3859 - val_accuracy: 0.8316\n",
      "Epoch 13/250\n",
      "28/28 [==============================] - 0s 985us/step - loss: 0.3501 - accuracy: 0.8321 - val_loss: 0.3825 - val_accuracy: 0.8316\n",
      "Epoch 14/250\n",
      "28/28 [==============================] - 0s 898us/step - loss: 0.3453 - accuracy: 0.8284 - val_loss: 0.3847 - val_accuracy: 0.8291\n",
      "Epoch 15/250\n",
      "28/28 [==============================] - 0s 916us/step - loss: 0.3401 - accuracy: 0.8366 - val_loss: 0.3830 - val_accuracy: 0.8316\n",
      "Epoch 16/250\n",
      "28/28 [==============================] - 0s 902us/step - loss: 0.3373 - accuracy: 0.8392 - val_loss: 0.3887 - val_accuracy: 0.8265\n",
      "Epoch 17/250\n",
      "28/28 [==============================] - 0s 910us/step - loss: 0.3361 - accuracy: 0.8369 - val_loss: 0.3827 - val_accuracy: 0.8189\n",
      "Epoch 18/250\n",
      "28/28 [==============================] - 0s 971us/step - loss: 0.3298 - accuracy: 0.8460 - val_loss: 0.3830 - val_accuracy: 0.8240\n",
      "Epoch 19/250\n",
      "28/28 [==============================] - 0s 975us/step - loss: 0.3273 - accuracy: 0.8432 - val_loss: 0.3825 - val_accuracy: 0.8291\n",
      "Epoch 20/250\n",
      "28/28 [==============================] - 0s 905us/step - loss: 0.3252 - accuracy: 0.8452 - val_loss: 0.3867 - val_accuracy: 0.8214\n",
      "Epoch 21/250\n",
      "28/28 [==============================] - 0s 913us/step - loss: 0.3227 - accuracy: 0.8477 - val_loss: 0.3799 - val_accuracy: 0.8316\n",
      "Epoch 22/250\n",
      "28/28 [==============================] - 0s 930us/step - loss: 0.3159 - accuracy: 0.8537 - val_loss: 0.3938 - val_accuracy: 0.8189\n",
      "Epoch 23/250\n",
      "28/28 [==============================] - 0s 920us/step - loss: 0.3142 - accuracy: 0.8463 - val_loss: 0.3761 - val_accuracy: 0.8291\n",
      "Epoch 24/250\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.3105 - accuracy: 0.8554 - val_loss: 0.3925 - val_accuracy: 0.8265\n",
      "Epoch 25/250\n",
      "28/28 [==============================] - 0s 922us/step - loss: 0.3060 - accuracy: 0.8545 - val_loss: 0.3877 - val_accuracy: 0.8316\n",
      "Epoch 26/250\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.3031 - accuracy: 0.8511 - val_loss: 0.3797 - val_accuracy: 0.8316\n",
      "Epoch 27/250\n",
      "28/28 [==============================] - 0s 913us/step - loss: 0.3018 - accuracy: 0.8625 - val_loss: 0.3800 - val_accuracy: 0.8342\n",
      "Epoch 28/250\n",
      "28/28 [==============================] - 0s 969us/step - loss: 0.3037 - accuracy: 0.8565 - val_loss: 0.4053 - val_accuracy: 0.8163\n",
      "Epoch 29/250\n",
      "28/28 [==============================] - 0s 919us/step - loss: 0.2942 - accuracy: 0.8613 - val_loss: 0.3781 - val_accuracy: 0.8316\n",
      "Epoch 30/250\n",
      "28/28 [==============================] - 0s 933us/step - loss: 0.2880 - accuracy: 0.8627 - val_loss: 0.3957 - val_accuracy: 0.8240\n",
      "Epoch 31/250\n",
      "28/28 [==============================] - 0s 911us/step - loss: 0.2857 - accuracy: 0.8693 - val_loss: 0.3817 - val_accuracy: 0.8316\n",
      "Epoch 32/250\n",
      "28/28 [==============================] - 0s 912us/step - loss: 0.2823 - accuracy: 0.8684 - val_loss: 0.3814 - val_accuracy: 0.8367\n",
      "Epoch 33/250\n",
      "28/28 [==============================] - 0s 934us/step - loss: 0.2760 - accuracy: 0.8718 - val_loss: 0.3868 - val_accuracy: 0.8265\n",
      "Epoch 34/250\n",
      "28/28 [==============================] - 0s 898us/step - loss: 0.2739 - accuracy: 0.8738 - val_loss: 0.4009 - val_accuracy: 0.8265\n",
      "Epoch 35/250\n",
      "28/28 [==============================] - 0s 918us/step - loss: 0.2711 - accuracy: 0.8707 - val_loss: 0.3834 - val_accuracy: 0.8240\n",
      "Epoch 36/250\n",
      "28/28 [==============================] - 0s 940us/step - loss: 0.2670 - accuracy: 0.8798 - val_loss: 0.3888 - val_accuracy: 0.8316\n",
      "Epoch 37/250\n",
      "28/28 [==============================] - 0s 904us/step - loss: 0.2641 - accuracy: 0.8798 - val_loss: 0.3922 - val_accuracy: 0.8393\n",
      "Epoch 38/250\n",
      "28/28 [==============================] - 0s 896us/step - loss: 0.2564 - accuracy: 0.8832 - val_loss: 0.3934 - val_accuracy: 0.8316\n",
      "Epoch 39/250\n",
      "28/28 [==============================] - 0s 888us/step - loss: 0.2561 - accuracy: 0.8843 - val_loss: 0.4053 - val_accuracy: 0.8163\n",
      "Epoch 40/250\n",
      "28/28 [==============================] - 0s 873us/step - loss: 0.2543 - accuracy: 0.8866 - val_loss: 0.4146 - val_accuracy: 0.8036\n",
      "Epoch 41/250\n",
      "28/28 [==============================] - 0s 909us/step - loss: 0.2513 - accuracy: 0.8871 - val_loss: 0.3915 - val_accuracy: 0.8189\n",
      "Epoch 42/250\n",
      "28/28 [==============================] - 0s 885us/step - loss: 0.2431 - accuracy: 0.8939 - val_loss: 0.4054 - val_accuracy: 0.8291\n",
      "Epoch 43/250\n",
      "28/28 [==============================] - 0s 901us/step - loss: 0.2415 - accuracy: 0.8979 - val_loss: 0.3987 - val_accuracy: 0.8316\n",
      "Epoch 44/250\n",
      "28/28 [==============================] - 0s 901us/step - loss: 0.2383 - accuracy: 0.8942 - val_loss: 0.4112 - val_accuracy: 0.8036\n",
      "Epoch 45/250\n",
      "28/28 [==============================] - 0s 880us/step - loss: 0.2314 - accuracy: 0.9036 - val_loss: 0.4176 - val_accuracy: 0.8112\n",
      "Epoch 46/250\n",
      "28/28 [==============================] - 0s 891us/step - loss: 0.2285 - accuracy: 0.8993 - val_loss: 0.4195 - val_accuracy: 0.8265\n",
      "Epoch 47/250\n",
      "28/28 [==============================] - 0s 894us/step - loss: 0.2251 - accuracy: 0.9073 - val_loss: 0.4281 - val_accuracy: 0.8214\n",
      "Epoch 48/250\n",
      "28/28 [==============================] - 0s 881us/step - loss: 0.2270 - accuracy: 0.9022 - val_loss: 0.4007 - val_accuracy: 0.8342\n",
      "Epoch 49/250\n",
      "28/28 [==============================] - 0s 891us/step - loss: 0.2315 - accuracy: 0.9036 - val_loss: 0.4014 - val_accuracy: 0.8316\n",
      "Epoch 50/250\n",
      "28/28 [==============================] - 0s 861us/step - loss: 0.2160 - accuracy: 0.9061 - val_loss: 0.4114 - val_accuracy: 0.8316\n",
      "Epoch 51/250\n",
      "28/28 [==============================] - 0s 902us/step - loss: 0.2159 - accuracy: 0.9095 - val_loss: 0.4132 - val_accuracy: 0.8316\n",
      "Epoch 52/250\n",
      "28/28 [==============================] - 0s 880us/step - loss: 0.2061 - accuracy: 0.9192 - val_loss: 0.4466 - val_accuracy: 0.8112\n",
      "Epoch 53/250\n",
      "28/28 [==============================] - 0s 900us/step - loss: 0.2088 - accuracy: 0.9146 - val_loss: 0.4194 - val_accuracy: 0.8495\n",
      "Epoch 54/250\n",
      "28/28 [==============================] - 0s 895us/step - loss: 0.2151 - accuracy: 0.9058 - val_loss: 0.4178 - val_accuracy: 0.8367\n",
      "Epoch 55/250\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.2040 - accuracy: 0.9163 - val_loss: 0.4553 - val_accuracy: 0.8214\n",
      "Epoch 56/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9197 - val_loss: 0.4482 - val_accuracy: 0.8240\n",
      "Epoch 57/250\n",
      "28/28 [==============================] - 0s 910us/step - loss: 0.2042 - accuracy: 0.9124 - val_loss: 0.4251 - val_accuracy: 0.8469\n",
      "Epoch 58/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 873us/step - loss: 0.1993 - accuracy: 0.9146 - val_loss: 0.4322 - val_accuracy: 0.8291\n",
      "Epoch 59/250\n",
      "28/28 [==============================] - 0s 914us/step - loss: 0.1904 - accuracy: 0.9226 - val_loss: 0.4492 - val_accuracy: 0.8265\n",
      "Epoch 60/250\n",
      "28/28 [==============================] - 0s 870us/step - loss: 0.1890 - accuracy: 0.9248 - val_loss: 0.4436 - val_accuracy: 0.8316\n",
      "Epoch 61/250\n",
      "28/28 [==============================] - 0s 880us/step - loss: 0.1841 - accuracy: 0.9271 - val_loss: 0.4462 - val_accuracy: 0.8265\n",
      "Epoch 62/250\n",
      "28/28 [==============================] - 0s 867us/step - loss: 0.1844 - accuracy: 0.9265 - val_loss: 0.4414 - val_accuracy: 0.8444\n",
      "Epoch 63/250\n",
      "28/28 [==============================] - 0s 852us/step - loss: 0.1787 - accuracy: 0.9277 - val_loss: 0.4427 - val_accuracy: 0.8520\n",
      "Epoch 64/250\n",
      "28/28 [==============================] - 0s 877us/step - loss: 0.1772 - accuracy: 0.9268 - val_loss: 0.4760 - val_accuracy: 0.8342\n",
      "Epoch 65/250\n",
      "28/28 [==============================] - 0s 888us/step - loss: 0.1769 - accuracy: 0.9274 - val_loss: 0.4641 - val_accuracy: 0.8444\n",
      "Epoch 66/250\n",
      "28/28 [==============================] - 0s 867us/step - loss: 0.1733 - accuracy: 0.9285 - val_loss: 0.4542 - val_accuracy: 0.8291\n",
      "Epoch 67/250\n",
      "28/28 [==============================] - 0s 863us/step - loss: 0.1662 - accuracy: 0.9348 - val_loss: 0.4520 - val_accuracy: 0.8393\n",
      "Epoch 68/250\n",
      "28/28 [==============================] - 0s 866us/step - loss: 0.1665 - accuracy: 0.9353 - val_loss: 0.4744 - val_accuracy: 0.8316\n",
      "Epoch 69/250\n",
      "28/28 [==============================] - 0s 928us/step - loss: 0.1667 - accuracy: 0.9317 - val_loss: 0.4755 - val_accuracy: 0.8342\n",
      "Epoch 70/250\n",
      "28/28 [==============================] - 0s 907us/step - loss: 0.1696 - accuracy: 0.9325 - val_loss: 0.4630 - val_accuracy: 0.8342\n",
      "Epoch 71/250\n",
      "28/28 [==============================] - 0s 900us/step - loss: 0.1590 - accuracy: 0.9365 - val_loss: 0.4604 - val_accuracy: 0.8444\n",
      "Epoch 72/250\n",
      "28/28 [==============================] - 0s 866us/step - loss: 0.1595 - accuracy: 0.9351 - val_loss: 0.4623 - val_accuracy: 0.8342\n",
      "Epoch 73/250\n",
      "28/28 [==============================] - 0s 986us/step - loss: 0.1617 - accuracy: 0.9382 - val_loss: 0.5151 - val_accuracy: 0.8214\n",
      "Epoch 74/250\n",
      "28/28 [==============================] - 0s 890us/step - loss: 0.1509 - accuracy: 0.9430 - val_loss: 0.4750 - val_accuracy: 0.8444\n",
      "Epoch 75/250\n",
      "28/28 [==============================] - 0s 865us/step - loss: 0.1474 - accuracy: 0.9455 - val_loss: 0.5242 - val_accuracy: 0.8163\n",
      "Epoch 76/250\n",
      "28/28 [==============================] - 0s 879us/step - loss: 0.1493 - accuracy: 0.9404 - val_loss: 0.4973 - val_accuracy: 0.8342\n",
      "Epoch 77/250\n",
      "28/28 [==============================] - 0s 917us/step - loss: 0.1476 - accuracy: 0.9470 - val_loss: 0.5009 - val_accuracy: 0.8495\n",
      "Epoch 78/250\n",
      "28/28 [==============================] - 0s 894us/step - loss: 0.1431 - accuracy: 0.9450 - val_loss: 0.4845 - val_accuracy: 0.8520\n",
      "Epoch 79/250\n",
      "28/28 [==============================] - 0s 956us/step - loss: 0.1433 - accuracy: 0.9433 - val_loss: 0.5413 - val_accuracy: 0.8189\n",
      "Epoch 80/250\n",
      "28/28 [==============================] - 0s 976us/step - loss: 0.1415 - accuracy: 0.9441 - val_loss: 0.5353 - val_accuracy: 0.8265\n",
      "Epoch 81/250\n",
      "28/28 [==============================] - 0s 980us/step - loss: 0.1361 - accuracy: 0.9464 - val_loss: 0.5750 - val_accuracy: 0.8112\n",
      "Epoch 82/250\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.1415 - accuracy: 0.9481 - val_loss: 0.5121 - val_accuracy: 0.8520\n",
      "Epoch 83/250\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.1303 - accuracy: 0.9521 - val_loss: 0.5409 - val_accuracy: 0.8520\n",
      "Epoch 84/250\n",
      "28/28 [==============================] - 0s 922us/step - loss: 0.1284 - accuracy: 0.9509 - val_loss: 0.5165 - val_accuracy: 0.8546\n",
      "Epoch 85/250\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.1294 - accuracy: 0.9518 - val_loss: 0.5351 - val_accuracy: 0.8316\n",
      "Epoch 86/250\n",
      "28/28 [==============================] - 0s 913us/step - loss: 0.1281 - accuracy: 0.9504 - val_loss: 0.5732 - val_accuracy: 0.8240\n",
      "Epoch 87/250\n",
      "28/28 [==============================] - 0s 934us/step - loss: 0.1347 - accuracy: 0.9495 - val_loss: 0.5669 - val_accuracy: 0.8367\n",
      "Epoch 88/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.1238 - accuracy: 0.9546 - val_loss: 0.5730 - val_accuracy: 0.8393\n",
      "Epoch 89/250\n",
      "28/28 [==============================] - 0s 936us/step - loss: 0.1216 - accuracy: 0.9532 - val_loss: 0.5640 - val_accuracy: 0.8418\n",
      "Epoch 90/250\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.1234 - accuracy: 0.9529 - val_loss: 0.5585 - val_accuracy: 0.8495\n",
      "Epoch 91/250\n",
      "28/28 [==============================] - 0s 904us/step - loss: 0.1310 - accuracy: 0.9467 - val_loss: 0.5497 - val_accuracy: 0.8444\n",
      "Epoch 92/250\n",
      "28/28 [==============================] - 0s 921us/step - loss: 0.1195 - accuracy: 0.9543 - val_loss: 0.5779 - val_accuracy: 0.8469\n",
      "Epoch 93/250\n",
      "28/28 [==============================] - 0s 969us/step - loss: 0.1154 - accuracy: 0.9566 - val_loss: 0.5787 - val_accuracy: 0.8367\n",
      "Epoch 94/250\n",
      "28/28 [==============================] - 0s 999us/step - loss: 0.1118 - accuracy: 0.9614 - val_loss: 0.5615 - val_accuracy: 0.8418\n",
      "Epoch 95/250\n",
      "28/28 [==============================] - 0s 943us/step - loss: 0.1135 - accuracy: 0.9597 - val_loss: 0.5972 - val_accuracy: 0.8240\n",
      "Epoch 96/250\n",
      "28/28 [==============================] - 0s 912us/step - loss: 0.1167 - accuracy: 0.9529 - val_loss: 0.5624 - val_accuracy: 0.8418\n",
      "Epoch 97/250\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.1119 - accuracy: 0.9586 - val_loss: 0.6089 - val_accuracy: 0.8342\n",
      "Epoch 98/250\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.1119 - accuracy: 0.9577 - val_loss: 0.5915 - val_accuracy: 0.8393\n",
      "Epoch 99/250\n",
      "28/28 [==============================] - 0s 874us/step - loss: 0.1065 - accuracy: 0.9631 - val_loss: 0.6193 - val_accuracy: 0.8393\n",
      "Epoch 100/250\n",
      "28/28 [==============================] - 0s 972us/step - loss: 0.1026 - accuracy: 0.9631 - val_loss: 0.6304 - val_accuracy: 0.8265\n",
      "Epoch 101/250\n",
      "28/28 [==============================] - 0s 932us/step - loss: 0.1028 - accuracy: 0.9645 - val_loss: 0.6197 - val_accuracy: 0.8316\n",
      "Epoch 102/250\n",
      "28/28 [==============================] - 0s 869us/step - loss: 0.1097 - accuracy: 0.9623 - val_loss: 0.6188 - val_accuracy: 0.8444\n",
      "Epoch 103/250\n",
      "28/28 [==============================] - 0s 863us/step - loss: 0.1068 - accuracy: 0.9600 - val_loss: 0.5847 - val_accuracy: 0.8571\n",
      "Epoch 104/250\n",
      "28/28 [==============================] - 0s 874us/step - loss: 0.1002 - accuracy: 0.9668 - val_loss: 0.6659 - val_accuracy: 0.8367\n",
      "Epoch 105/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.1032 - accuracy: 0.9623 - val_loss: 0.6490 - val_accuracy: 0.8367\n",
      "Epoch 106/250\n",
      "28/28 [==============================] - 0s 913us/step - loss: 0.1025 - accuracy: 0.9631 - val_loss: 0.6417 - val_accuracy: 0.8393\n",
      "Epoch 107/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9606 - val_loss: 0.6257 - val_accuracy: 0.8469\n",
      "Epoch 108/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9651 - val_loss: 0.6738 - val_accuracy: 0.8367\n",
      "Epoch 109/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9680 - val_loss: 0.6394 - val_accuracy: 0.8469\n",
      "Epoch 110/250\n",
      "28/28 [==============================] - 0s 936us/step - loss: 0.1024 - accuracy: 0.9626 - val_loss: 0.6449 - val_accuracy: 0.8495\n",
      "Epoch 111/250\n",
      "28/28 [==============================] - 0s 880us/step - loss: 0.1139 - accuracy: 0.9597 - val_loss: 0.6296 - val_accuracy: 0.8444\n",
      "Epoch 112/250\n",
      "28/28 [==============================] - 0s 954us/step - loss: 0.1016 - accuracy: 0.9600 - val_loss: 0.6787 - val_accuracy: 0.8520\n",
      "Epoch 113/250\n",
      "28/28 [==============================] - 0s 914us/step - loss: 0.0891 - accuracy: 0.9708 - val_loss: 0.6632 - val_accuracy: 0.8393\n",
      "Epoch 114/250\n",
      "28/28 [==============================] - 0s 877us/step - loss: 0.0898 - accuracy: 0.9694 - val_loss: 0.7010 - val_accuracy: 0.8316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/250\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.0868 - accuracy: 0.9708 - val_loss: 0.6731 - val_accuracy: 0.8469\n",
      "Epoch 116/250\n",
      "28/28 [==============================] - 0s 886us/step - loss: 0.0829 - accuracy: 0.9739 - val_loss: 0.6632 - val_accuracy: 0.8597\n",
      "Epoch 117/250\n",
      "28/28 [==============================] - 0s 922us/step - loss: 0.0839 - accuracy: 0.9719 - val_loss: 0.6853 - val_accuracy: 0.8495\n",
      "Epoch 118/250\n",
      "28/28 [==============================] - 0s 927us/step - loss: 0.0815 - accuracy: 0.9731 - val_loss: 0.7004 - val_accuracy: 0.8367\n",
      "Epoch 119/250\n",
      "28/28 [==============================] - 0s 916us/step - loss: 0.0780 - accuracy: 0.9731 - val_loss: 0.7328 - val_accuracy: 0.8265\n",
      "Epoch 120/250\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.0772 - accuracy: 0.9719 - val_loss: 0.7005 - val_accuracy: 0.8418\n",
      "Epoch 121/250\n",
      "28/28 [==============================] - 0s 918us/step - loss: 0.0856 - accuracy: 0.9668 - val_loss: 0.7358 - val_accuracy: 0.8393\n",
      "Epoch 122/250\n",
      "28/28 [==============================] - 0s 976us/step - loss: 0.0860 - accuracy: 0.9677 - val_loss: 0.7040 - val_accuracy: 0.8444\n",
      "Epoch 123/250\n",
      "28/28 [==============================] - 0s 899us/step - loss: 0.0813 - accuracy: 0.9705 - val_loss: 0.7071 - val_accuracy: 0.8520\n",
      "Epoch 124/250\n",
      "28/28 [==============================] - 0s 879us/step - loss: 0.0804 - accuracy: 0.9714 - val_loss: 0.7336 - val_accuracy: 0.8520\n",
      "Epoch 125/250\n",
      "28/28 [==============================] - 0s 981us/step - loss: 0.0817 - accuracy: 0.9722 - val_loss: 0.7238 - val_accuracy: 0.8520\n",
      "Epoch 126/250\n",
      "28/28 [==============================] - 0s 918us/step - loss: 0.0777 - accuracy: 0.9745 - val_loss: 0.7761 - val_accuracy: 0.8444\n",
      "Epoch 127/250\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.1011 - accuracy: 0.9648 - val_loss: 0.7125 - val_accuracy: 0.8495\n",
      "Epoch 128/250\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0812 - accuracy: 0.9708 - val_loss: 0.7380 - val_accuracy: 0.8316\n",
      "Epoch 129/250\n",
      "28/28 [==============================] - 0s 877us/step - loss: 0.0738 - accuracy: 0.9750 - val_loss: 0.7246 - val_accuracy: 0.8367\n",
      "Epoch 130/250\n",
      "28/28 [==============================] - 0s 859us/step - loss: 0.0694 - accuracy: 0.9762 - val_loss: 0.7365 - val_accuracy: 0.8316\n",
      "Epoch 131/250\n",
      "28/28 [==============================] - 0s 898us/step - loss: 0.0760 - accuracy: 0.9725 - val_loss: 0.7354 - val_accuracy: 0.8418\n",
      "Epoch 132/250\n",
      "28/28 [==============================] - 0s 874us/step - loss: 0.0704 - accuracy: 0.9770 - val_loss: 0.7784 - val_accuracy: 0.8367\n",
      "Epoch 133/250\n",
      "28/28 [==============================] - 0s 934us/step - loss: 0.0690 - accuracy: 0.9756 - val_loss: 0.7768 - val_accuracy: 0.8316\n",
      "Epoch 134/250\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.0654 - accuracy: 0.9753 - val_loss: 0.7795 - val_accuracy: 0.8367\n",
      "Epoch 135/250\n",
      "28/28 [==============================] - 0s 881us/step - loss: 0.0619 - accuracy: 0.9816 - val_loss: 0.7722 - val_accuracy: 0.8444\n",
      "Epoch 136/250\n",
      "28/28 [==============================] - 0s 972us/step - loss: 0.0606 - accuracy: 0.9816 - val_loss: 0.8021 - val_accuracy: 0.8520\n",
      "Epoch 137/250\n",
      "28/28 [==============================] - 0s 938us/step - loss: 0.0644 - accuracy: 0.9770 - val_loss: 0.7928 - val_accuracy: 0.8520\n",
      "Epoch 138/250\n",
      "28/28 [==============================] - 0s 932us/step - loss: 0.0678 - accuracy: 0.9784 - val_loss: 0.8618 - val_accuracy: 0.8291\n",
      "Epoch 139/250\n",
      "28/28 [==============================] - 0s 941us/step - loss: 0.0686 - accuracy: 0.9779 - val_loss: 0.7975 - val_accuracy: 0.8418\n",
      "Epoch 140/250\n",
      "28/28 [==============================] - 0s 853us/step - loss: 0.0638 - accuracy: 0.9762 - val_loss: 0.8513 - val_accuracy: 0.8418\n",
      "Epoch 141/250\n",
      "28/28 [==============================] - 0s 874us/step - loss: 0.0607 - accuracy: 0.9801 - val_loss: 0.8005 - val_accuracy: 0.8469\n",
      "Epoch 142/250\n",
      "28/28 [==============================] - 0s 872us/step - loss: 0.0547 - accuracy: 0.9844 - val_loss: 0.8724 - val_accuracy: 0.8367\n",
      "Epoch 143/250\n",
      "28/28 [==============================] - 0s 939us/step - loss: 0.0560 - accuracy: 0.9836 - val_loss: 0.8317 - val_accuracy: 0.8418\n",
      "Epoch 144/250\n",
      "28/28 [==============================] - 0s 874us/step - loss: 0.0534 - accuracy: 0.9827 - val_loss: 0.8660 - val_accuracy: 0.8520\n",
      "Epoch 145/250\n",
      "28/28 [==============================] - 0s 884us/step - loss: 0.0527 - accuracy: 0.9838 - val_loss: 0.8272 - val_accuracy: 0.8520\n",
      "Epoch 146/250\n",
      "28/28 [==============================] - 0s 905us/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.8648 - val_accuracy: 0.8469\n",
      "Epoch 147/250\n",
      "28/28 [==============================] - 0s 922us/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.8616 - val_accuracy: 0.8444\n",
      "Epoch 148/250\n",
      "28/28 [==============================] - 0s 950us/step - loss: 0.0662 - accuracy: 0.9762 - val_loss: 0.9047 - val_accuracy: 0.8393\n",
      "Epoch 149/250\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.0704 - accuracy: 0.9745 - val_loss: 0.8994 - val_accuracy: 0.8393\n",
      "Epoch 150/250\n",
      "28/28 [==============================] - 0s 860us/step - loss: 0.0648 - accuracy: 0.9776 - val_loss: 0.9146 - val_accuracy: 0.8367\n",
      "Epoch 151/250\n",
      "28/28 [==============================] - 0s 925us/step - loss: 0.0569 - accuracy: 0.9816 - val_loss: 0.8988 - val_accuracy: 0.8469\n",
      "Epoch 152/250\n",
      "28/28 [==============================] - 0s 888us/step - loss: 0.0504 - accuracy: 0.9858 - val_loss: 0.8722 - val_accuracy: 0.8495\n",
      "Epoch 153/250\n",
      "28/28 [==============================] - 0s 844us/step - loss: 0.0520 - accuracy: 0.9818 - val_loss: 0.9144 - val_accuracy: 0.8316\n",
      "Epoch 154/250\n",
      "28/28 [==============================] - 0s 862us/step - loss: 0.0520 - accuracy: 0.9847 - val_loss: 0.8858 - val_accuracy: 0.8469\n",
      "Epoch 155/250\n",
      "28/28 [==============================] - 0s 865us/step - loss: 0.0463 - accuracy: 0.9870 - val_loss: 0.9158 - val_accuracy: 0.8495\n",
      "Epoch 156/250\n",
      "28/28 [==============================] - 0s 891us/step - loss: 0.0538 - accuracy: 0.9810 - val_loss: 0.9110 - val_accuracy: 0.8546\n",
      "Epoch 157/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.9861 - val_loss: 0.9294 - val_accuracy: 0.8495\n",
      "Epoch 158/250\n",
      "28/28 [==============================] - 0s 909us/step - loss: 0.0485 - accuracy: 0.9844 - val_loss: 0.8940 - val_accuracy: 0.8495\n",
      "Epoch 159/250\n",
      "28/28 [==============================] - 0s 957us/step - loss: 0.0463 - accuracy: 0.9850 - val_loss: 0.9323 - val_accuracy: 0.8469\n",
      "Epoch 160/250\n",
      "28/28 [==============================] - 0s 931us/step - loss: 0.0448 - accuracy: 0.9872 - val_loss: 0.9544 - val_accuracy: 0.8444\n",
      "Epoch 161/250\n",
      "28/28 [==============================] - 0s 890us/step - loss: 0.0415 - accuracy: 0.9892 - val_loss: 0.9597 - val_accuracy: 0.8495\n",
      "Epoch 162/250\n",
      "28/28 [==============================] - 0s 882us/step - loss: 0.0435 - accuracy: 0.9878 - val_loss: 0.9716 - val_accuracy: 0.8342\n",
      "Epoch 163/250\n",
      "28/28 [==============================] - 0s 859us/step - loss: 0.0439 - accuracy: 0.9858 - val_loss: 0.9238 - val_accuracy: 0.8571\n",
      "Epoch 164/250\n",
      "28/28 [==============================] - 0s 886us/step - loss: 0.0510 - accuracy: 0.9813 - val_loss: 1.0103 - val_accuracy: 0.8393\n",
      "Epoch 165/250\n",
      "28/28 [==============================] - 0s 864us/step - loss: 0.0456 - accuracy: 0.9838 - val_loss: 0.9340 - val_accuracy: 0.8622\n",
      "Epoch 166/250\n",
      "28/28 [==============================] - 0s 896us/step - loss: 0.0436 - accuracy: 0.9855 - val_loss: 0.9830 - val_accuracy: 0.8571\n",
      "Epoch 167/250\n",
      "28/28 [==============================] - 0s 853us/step - loss: 0.0455 - accuracy: 0.9833 - val_loss: 0.9725 - val_accuracy: 0.8393\n",
      "Epoch 168/250\n",
      "28/28 [==============================] - 0s 883us/step - loss: 0.0430 - accuracy: 0.9867 - val_loss: 0.9950 - val_accuracy: 0.8469\n",
      "Epoch 169/250\n",
      "28/28 [==============================] - 0s 856us/step - loss: 0.0617 - accuracy: 0.9813 - val_loss: 1.0176 - val_accuracy: 0.8393\n",
      "Epoch 170/250\n",
      "28/28 [==============================] - 0s 895us/step - loss: 0.0625 - accuracy: 0.9756 - val_loss: 0.9915 - val_accuracy: 0.8520\n",
      "Epoch 171/250\n",
      "28/28 [==============================] - 0s 856us/step - loss: 0.0617 - accuracy: 0.9784 - val_loss: 1.0718 - val_accuracy: 0.8393\n",
      "Epoch 172/250\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.0615 - accuracy: 0.9779 - val_loss: 0.9996 - val_accuracy: 0.8495\n",
      "Epoch 173/250\n",
      "28/28 [==============================] - 0s 865us/step - loss: 0.0442 - accuracy: 0.9847 - val_loss: 1.0289 - val_accuracy: 0.8214\n",
      "Epoch 174/250\n",
      "28/28 [==============================] - 0s 896us/step - loss: 0.0566 - accuracy: 0.9782 - val_loss: 0.9960 - val_accuracy: 0.8546\n",
      "Epoch 175/250\n",
      "28/28 [==============================] - 0s 911us/step - loss: 0.0546 - accuracy: 0.9816 - val_loss: 1.0027 - val_accuracy: 0.8393\n",
      "Epoch 176/250\n",
      "28/28 [==============================] - 0s 911us/step - loss: 0.0515 - accuracy: 0.9801 - val_loss: 0.9692 - val_accuracy: 0.8418\n",
      "Epoch 177/250\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.0457 - accuracy: 0.9833 - val_loss: 1.0365 - val_accuracy: 0.8316\n",
      "Epoch 178/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0489 - accuracy: 0.9861 - val_loss: 1.0426 - val_accuracy: 0.8418\n",
      "Epoch 179/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9767 - val_loss: 1.0053 - val_accuracy: 0.8265\n",
      "Epoch 180/250\n",
      "28/28 [==============================] - 0s 991us/step - loss: 0.0424 - accuracy: 0.9881 - val_loss: 1.0271 - val_accuracy: 0.8444\n",
      "Epoch 181/250\n",
      "28/28 [==============================] - 0s 873us/step - loss: 0.0317 - accuracy: 0.9915 - val_loss: 1.0956 - val_accuracy: 0.8393\n",
      "Epoch 182/250\n",
      "28/28 [==============================] - 0s 864us/step - loss: 0.0336 - accuracy: 0.9909 - val_loss: 1.0721 - val_accuracy: 0.8418\n",
      "Epoch 183/250\n",
      "28/28 [==============================] - 0s 839us/step - loss: 0.0309 - accuracy: 0.9932 - val_loss: 1.0573 - val_accuracy: 0.8469\n",
      "Epoch 184/250\n",
      "28/28 [==============================] - 0s 861us/step - loss: 0.0282 - accuracy: 0.9938 - val_loss: 1.0569 - val_accuracy: 0.8444\n",
      "Epoch 185/250\n",
      "28/28 [==============================] - 0s 873us/step - loss: 0.0300 - accuracy: 0.9918 - val_loss: 1.0794 - val_accuracy: 0.8520\n",
      "Epoch 186/250\n",
      "28/28 [==============================] - 0s 976us/step - loss: 0.0301 - accuracy: 0.9923 - val_loss: 1.1160 - val_accuracy: 0.8444\n",
      "Epoch 187/250\n",
      "28/28 [==============================] - 0s 933us/step - loss: 0.0292 - accuracy: 0.9929 - val_loss: 1.0907 - val_accuracy: 0.8597\n",
      "Epoch 188/250\n",
      "28/28 [==============================] - 0s 866us/step - loss: 0.0275 - accuracy: 0.9935 - val_loss: 1.1266 - val_accuracy: 0.8546\n",
      "Epoch 189/250\n",
      "28/28 [==============================] - 0s 871us/step - loss: 0.0292 - accuracy: 0.9932 - val_loss: 1.1022 - val_accuracy: 0.8469\n",
      "Epoch 190/250\n",
      "28/28 [==============================] - 0s 855us/step - loss: 0.0273 - accuracy: 0.9952 - val_loss: 1.1033 - val_accuracy: 0.8546\n",
      "Epoch 191/250\n",
      "28/28 [==============================] - 0s 842us/step - loss: 0.0267 - accuracy: 0.9935 - val_loss: 1.1121 - val_accuracy: 0.8444\n",
      "Epoch 192/250\n",
      "28/28 [==============================] - 0s 855us/step - loss: 0.0289 - accuracy: 0.9921 - val_loss: 1.1076 - val_accuracy: 0.8546\n",
      "Epoch 193/250\n",
      "28/28 [==============================] - 0s 965us/step - loss: 0.0333 - accuracy: 0.9895 - val_loss: 1.1285 - val_accuracy: 0.8495\n",
      "Epoch 194/250\n",
      "28/28 [==============================] - 0s 932us/step - loss: 0.0349 - accuracy: 0.9881 - val_loss: 1.1750 - val_accuracy: 0.8316\n",
      "Epoch 195/250\n",
      "28/28 [==============================] - 0s 901us/step - loss: 0.0293 - accuracy: 0.9906 - val_loss: 1.1989 - val_accuracy: 0.8393\n",
      "Epoch 196/250\n",
      "28/28 [==============================] - 0s 903us/step - loss: 0.0249 - accuracy: 0.9932 - val_loss: 1.1953 - val_accuracy: 0.8418\n",
      "Epoch 197/250\n",
      "28/28 [==============================] - 0s 948us/step - loss: 0.0274 - accuracy: 0.9935 - val_loss: 1.2026 - val_accuracy: 0.8546\n",
      "Epoch 198/250\n",
      "28/28 [==============================] - 0s 915us/step - loss: 0.0244 - accuracy: 0.9943 - val_loss: 1.1827 - val_accuracy: 0.8495\n",
      "Epoch 199/250\n",
      "28/28 [==============================] - 0s 872us/step - loss: 0.0250 - accuracy: 0.9940 - val_loss: 1.1712 - val_accuracy: 0.8520\n",
      "Epoch 200/250\n",
      "28/28 [==============================] - 0s 912us/step - loss: 0.0218 - accuracy: 0.9957 - val_loss: 1.1955 - val_accuracy: 0.8444\n",
      "Epoch 201/250\n",
      "28/28 [==============================] - 0s 916us/step - loss: 0.0228 - accuracy: 0.9957 - val_loss: 1.1867 - val_accuracy: 0.8520\n",
      "Epoch 202/250\n",
      "28/28 [==============================] - 0s 896us/step - loss: 0.0254 - accuracy: 0.9921 - val_loss: 1.2208 - val_accuracy: 0.8546\n",
      "Epoch 203/250\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.0282 - accuracy: 0.9926 - val_loss: 1.1922 - val_accuracy: 0.8418\n",
      "Epoch 204/250\n",
      "28/28 [==============================] - 0s 895us/step - loss: 0.0328 - accuracy: 0.9901 - val_loss: 1.1686 - val_accuracy: 0.8469\n",
      "Epoch 205/250\n",
      "28/28 [==============================] - 0s 903us/step - loss: 0.0387 - accuracy: 0.9887 - val_loss: 1.2123 - val_accuracy: 0.8469\n",
      "Epoch 206/250\n",
      "28/28 [==============================] - 0s 903us/step - loss: 0.0367 - accuracy: 0.9875 - val_loss: 1.2611 - val_accuracy: 0.8495\n",
      "Epoch 207/250\n",
      "28/28 [==============================] - 0s 879us/step - loss: 0.0320 - accuracy: 0.9904 - val_loss: 1.1636 - val_accuracy: 0.8520\n",
      "Epoch 208/250\n",
      "28/28 [==============================] - 0s 924us/step - loss: 0.0295 - accuracy: 0.9901 - val_loss: 1.1872 - val_accuracy: 0.8520\n",
      "Epoch 209/250\n",
      "28/28 [==============================] - 0s 870us/step - loss: 0.0422 - accuracy: 0.9872 - val_loss: 1.2149 - val_accuracy: 0.8520\n",
      "Epoch 210/250\n",
      "28/28 [==============================] - 0s 917us/step - loss: 0.0608 - accuracy: 0.9818 - val_loss: 1.2859 - val_accuracy: 0.8418\n",
      "Epoch 211/250\n",
      "28/28 [==============================] - 0s 884us/step - loss: 0.0582 - accuracy: 0.9782 - val_loss: 1.2368 - val_accuracy: 0.8316\n",
      "Epoch 212/250\n",
      "28/28 [==============================] - 0s 909us/step - loss: 0.0392 - accuracy: 0.9864 - val_loss: 1.2361 - val_accuracy: 0.8520\n",
      "Epoch 213/250\n",
      "28/28 [==============================] - 0s 891us/step - loss: 0.0242 - accuracy: 0.9940 - val_loss: 1.2080 - val_accuracy: 0.8520\n",
      "Epoch 214/250\n",
      "28/28 [==============================] - 0s 893us/step - loss: 0.0208 - accuracy: 0.9955 - val_loss: 1.2445 - val_accuracy: 0.8571\n",
      "Epoch 215/250\n",
      "28/28 [==============================] - 0s 894us/step - loss: 0.0209 - accuracy: 0.9955 - val_loss: 1.2485 - val_accuracy: 0.8546\n",
      "Epoch 216/250\n",
      "28/28 [==============================] - 0s 874us/step - loss: 0.0200 - accuracy: 0.9952 - val_loss: 1.2615 - val_accuracy: 0.8520\n",
      "Epoch 217/250\n",
      "28/28 [==============================] - 0s 896us/step - loss: 0.0169 - accuracy: 0.9972 - val_loss: 1.2920 - val_accuracy: 0.8546\n",
      "Epoch 218/250\n",
      "28/28 [==============================] - 0s 872us/step - loss: 0.0162 - accuracy: 0.9972 - val_loss: 1.2850 - val_accuracy: 0.8418\n",
      "Epoch 219/250\n",
      "28/28 [==============================] - 0s 903us/step - loss: 0.0170 - accuracy: 0.9969 - val_loss: 1.3007 - val_accuracy: 0.8571\n",
      "Epoch 220/250\n",
      "28/28 [==============================] - 0s 867us/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 1.3255 - val_accuracy: 0.8469\n",
      "Epoch 221/250\n",
      "28/28 [==============================] - 0s 891us/step - loss: 0.0186 - accuracy: 0.9966 - val_loss: 1.3084 - val_accuracy: 0.8495\n",
      "Epoch 222/250\n",
      "28/28 [==============================] - 0s 866us/step - loss: 0.0183 - accuracy: 0.9966 - val_loss: 1.3011 - val_accuracy: 0.8571\n",
      "Epoch 223/250\n",
      "28/28 [==============================] - 0s 894us/step - loss: 0.0162 - accuracy: 0.9977 - val_loss: 1.3406 - val_accuracy: 0.8418\n",
      "Epoch 224/250\n",
      "28/28 [==============================] - 0s 870us/step - loss: 0.0149 - accuracy: 0.9977 - val_loss: 1.3458 - val_accuracy: 0.8520\n",
      "Epoch 225/250\n",
      "28/28 [==============================] - 0s 904us/step - loss: 0.0193 - accuracy: 0.9957 - val_loss: 1.3165 - val_accuracy: 0.8495\n",
      "Epoch 226/250\n",
      "28/28 [==============================] - 0s 905us/step - loss: 0.0163 - accuracy: 0.9963 - val_loss: 1.3572 - val_accuracy: 0.8444\n",
      "Epoch 227/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 882us/step - loss: 0.0188 - accuracy: 0.9966 - val_loss: 1.3548 - val_accuracy: 0.8546\n",
      "Epoch 228/250\n",
      "28/28 [==============================] - 0s 874us/step - loss: 0.0151 - accuracy: 0.9972 - val_loss: 1.3711 - val_accuracy: 0.8597\n",
      "Epoch 229/250\n",
      "28/28 [==============================] - 0s 897us/step - loss: 0.0154 - accuracy: 0.9974 - val_loss: 1.3773 - val_accuracy: 0.8495\n",
      "Epoch 230/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0166 - accuracy: 0.9977 - val_loss: 1.4332 - val_accuracy: 0.8520\n",
      "Epoch 231/250\n",
      "28/28 [==============================] - 0s 931us/step - loss: 0.0155 - accuracy: 0.9972 - val_loss: 1.3680 - val_accuracy: 0.8469\n",
      "Epoch 232/250\n",
      "28/28 [==============================] - 0s 896us/step - loss: 0.0157 - accuracy: 0.9966 - val_loss: 1.3563 - val_accuracy: 0.8520\n",
      "Epoch 233/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9943 - val_loss: 1.3418 - val_accuracy: 0.8597\n",
      "Epoch 234/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0550 - accuracy: 0.9838 - val_loss: 1.4177 - val_accuracy: 0.8393\n",
      "Epoch 235/250\n",
      "28/28 [==============================] - 0s 925us/step - loss: 0.0960 - accuracy: 0.9654 - val_loss: 1.5129 - val_accuracy: 0.8163\n",
      "Epoch 236/250\n",
      "28/28 [==============================] - 0s 959us/step - loss: 0.1105 - accuracy: 0.9674 - val_loss: 1.4911 - val_accuracy: 0.8138\n",
      "Epoch 237/250\n",
      "28/28 [==============================] - 0s 882us/step - loss: 0.1115 - accuracy: 0.9580 - val_loss: 1.2759 - val_accuracy: 0.8597\n",
      "Epoch 238/250\n",
      "28/28 [==============================] - 0s 872us/step - loss: 0.0554 - accuracy: 0.9824 - val_loss: 1.3432 - val_accuracy: 0.8469\n",
      "Epoch 239/250\n",
      "28/28 [==============================] - 0s 873us/step - loss: 0.0362 - accuracy: 0.9887 - val_loss: 1.3026 - val_accuracy: 0.8367\n",
      "Epoch 240/250\n",
      "28/28 [==============================] - 0s 874us/step - loss: 0.0319 - accuracy: 0.9918 - val_loss: 1.2713 - val_accuracy: 0.8546\n",
      "Epoch 241/250\n",
      "28/28 [==============================] - 0s 861us/step - loss: 0.0257 - accuracy: 0.9926 - val_loss: 1.2046 - val_accuracy: 0.8469\n",
      "Epoch 242/250\n",
      "28/28 [==============================] - 0s 884us/step - loss: 0.0178 - accuracy: 0.9966 - val_loss: 1.3012 - val_accuracy: 0.8546\n",
      "Epoch 243/250\n",
      "28/28 [==============================] - 0s 874us/step - loss: 0.0148 - accuracy: 0.9977 - val_loss: 1.3191 - val_accuracy: 0.8495\n",
      "Epoch 244/250\n",
      "28/28 [==============================] - 0s 907us/step - loss: 0.0146 - accuracy: 0.9980 - val_loss: 1.3309 - val_accuracy: 0.8495\n",
      "Epoch 245/250\n",
      "28/28 [==============================] - 0s 882us/step - loss: 0.0135 - accuracy: 0.9972 - val_loss: 1.3591 - val_accuracy: 0.8469\n",
      "Epoch 246/250\n",
      "28/28 [==============================] - 0s 917us/step - loss: 0.0146 - accuracy: 0.9972 - val_loss: 1.3780 - val_accuracy: 0.8469\n",
      "Epoch 247/250\n",
      "28/28 [==============================] - 0s 864us/step - loss: 0.0144 - accuracy: 0.9972 - val_loss: 1.3588 - val_accuracy: 0.8546\n",
      "Epoch 248/250\n",
      "28/28 [==============================] - 0s 879us/step - loss: 0.0130 - accuracy: 0.9980 - val_loss: 1.3867 - val_accuracy: 0.8495\n",
      "Epoch 249/250\n",
      "28/28 [==============================] - 0s 907us/step - loss: 0.0124 - accuracy: 0.9980 - val_loss: 1.3823 - val_accuracy: 0.8571\n",
      "Epoch 250/250\n",
      "28/28 [==============================] - 0s 921us/step - loss: 0.0114 - accuracy: 0.9986 - val_loss: 1.3900 - val_accuracy: 0.8495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28d30eee0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DNN_2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_DNN_2.fit(data_train_features_norm, keras.utils.to_categorical(train_label_values,num_classes), \n",
    "              batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "#Overfitting, the training and validation sets are very different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3485f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.2608349323272705\n",
      "Test accuracy: 0.8459183573722839\n"
     ]
    }
   ],
   "source": [
    "score_2 = model_DNN_2.evaluate(data_test_features_norm, keras.utils.to_categorical(test_label_values, num_classes), verbose=0)\n",
    "print(\"Test loss:\", score_2[0])\n",
    "print(\"Test accuracy:\", score_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6ea61db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_6 (Flatten)         (None, 11)                0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 64)                768       \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,178\n",
      "Trainable params: 34,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Four hidden layers + softmax + canonical\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "model_DNN_reg = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=11),   \n",
    "        layers.Flatten(),   \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_DNN_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "972412d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "28/28 [==============================] - 1s 6ms/step - loss: 18.2524 - accuracy: 0.7782 - val_loss: 15.5580 - val_accuracy: 0.7934\n",
      "Epoch 2/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 13.4702 - accuracy: 0.7805 - val_loss: 11.2982 - val_accuracy: 0.7934\n",
      "Epoch 3/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 9.6111 - accuracy: 0.7816 - val_loss: 7.8852 - val_accuracy: 0.7959\n",
      "Epoch 4/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 6.5689 - accuracy: 0.8023 - val_loss: 5.2539 - val_accuracy: 0.8061\n",
      "Epoch 5/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 4.2682 - accuracy: 0.8035 - val_loss: 3.2950 - val_accuracy: 0.8087\n",
      "Epoch 6/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 2.5786 - accuracy: 0.8066 - val_loss: 1.9100 - val_accuracy: 0.8010\n",
      "Epoch 7/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4642 - accuracy: 0.8035 - val_loss: 1.0674 - val_accuracy: 0.7959\n",
      "Epoch 8/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.8589 - accuracy: 0.7898 - val_loss: 0.7098 - val_accuracy: 0.7857\n",
      "Epoch 9/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6571 - accuracy: 0.7941 - val_loss: 0.6183 - val_accuracy: 0.7934\n",
      "Epoch 10/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.6017 - accuracy: 0.8020 - val_loss: 0.5902 - val_accuracy: 0.7985\n",
      "Epoch 11/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.8018 - val_loss: 0.5764 - val_accuracy: 0.8010\n",
      "Epoch 12/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5566 - accuracy: 0.8018 - val_loss: 0.5549 - val_accuracy: 0.8112\n",
      "Epoch 13/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5557 - accuracy: 0.8057 - val_loss: 0.5502 - val_accuracy: 0.8087\n",
      "Epoch 14/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.8020 - val_loss: 0.5487 - val_accuracy: 0.7985\n",
      "Epoch 15/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.5311 - accuracy: 0.8103 - val_loss: 0.5385 - val_accuracy: 0.8061\n",
      "Epoch 16/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.5273 - accuracy: 0.8074 - val_loss: 0.5478 - val_accuracy: 0.7985\n",
      "Epoch 17/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5448 - accuracy: 0.8003 - val_loss: 0.5287 - val_accuracy: 0.8189\n",
      "Epoch 18/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.8037 - val_loss: 0.5217 - val_accuracy: 0.8112\n",
      "Epoch 19/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.8069 - val_loss: 0.5176 - val_accuracy: 0.8163\n",
      "Epoch 20/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.8032 - val_loss: 0.5252 - val_accuracy: 0.8112\n",
      "Epoch 21/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.8100 - val_loss: 0.5245 - val_accuracy: 0.7883\n",
      "Epoch 22/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.8091 - val_loss: 0.5231 - val_accuracy: 0.8010\n",
      "Epoch 23/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.8120 - val_loss: 0.5066 - val_accuracy: 0.8138\n",
      "Epoch 24/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.8103 - val_loss: 0.5101 - val_accuracy: 0.8087\n",
      "Epoch 25/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4925 - accuracy: 0.8046 - val_loss: 0.5081 - val_accuracy: 0.7959\n",
      "Epoch 26/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.8083 - val_loss: 0.5403 - val_accuracy: 0.7883\n",
      "Epoch 27/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.8100 - val_loss: 0.5079 - val_accuracy: 0.8112\n",
      "Epoch 28/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.8097 - val_loss: 0.5162 - val_accuracy: 0.7934\n",
      "Epoch 29/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.8086 - val_loss: 0.5469 - val_accuracy: 0.7679\n",
      "Epoch 30/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4956 - accuracy: 0.8020 - val_loss: 0.4991 - val_accuracy: 0.7908\n",
      "Epoch 31/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4807 - accuracy: 0.8120 - val_loss: 0.4998 - val_accuracy: 0.8061\n",
      "Epoch 32/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4751 - accuracy: 0.8111 - val_loss: 0.4936 - val_accuracy: 0.8061\n",
      "Epoch 33/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.8120 - val_loss: 0.5218 - val_accuracy: 0.8036\n",
      "Epoch 34/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.8142 - val_loss: 0.4914 - val_accuracy: 0.8061\n",
      "Epoch 35/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.8117 - val_loss: 0.4927 - val_accuracy: 0.8087\n",
      "Epoch 36/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.8142 - val_loss: 0.4957 - val_accuracy: 0.8087\n",
      "Epoch 37/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.8020 - val_loss: 0.5006 - val_accuracy: 0.7959\n",
      "Epoch 38/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4729 - accuracy: 0.8097 - val_loss: 0.4927 - val_accuracy: 0.7959\n",
      "Epoch 39/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4698 - accuracy: 0.8114 - val_loss: 0.5047 - val_accuracy: 0.7781\n",
      "Epoch 40/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.8148 - val_loss: 0.4950 - val_accuracy: 0.7908\n",
      "Epoch 41/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.8142 - val_loss: 0.4870 - val_accuracy: 0.8061\n",
      "Epoch 42/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4610 - accuracy: 0.8108 - val_loss: 0.4828 - val_accuracy: 0.7985\n",
      "Epoch 43/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.8108 - val_loss: 0.4789 - val_accuracy: 0.8061\n",
      "Epoch 44/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.8111 - val_loss: 0.4856 - val_accuracy: 0.7883\n",
      "Epoch 45/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.8120 - val_loss: 0.4797 - val_accuracy: 0.8036\n",
      "Epoch 46/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.8168 - val_loss: 0.4963 - val_accuracy: 0.7730\n",
      "Epoch 47/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.8151 - val_loss: 0.4847 - val_accuracy: 0.7883\n",
      "Epoch 48/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.8145 - val_loss: 0.4821 - val_accuracy: 0.8138\n",
      "Epoch 49/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4553 - accuracy: 0.8205 - val_loss: 0.4777 - val_accuracy: 0.8061\n",
      "Epoch 50/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4579 - accuracy: 0.8157 - val_loss: 0.4781 - val_accuracy: 0.8061\n",
      "Epoch 51/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4555 - accuracy: 0.8168 - val_loss: 0.4741 - val_accuracy: 0.8138\n",
      "Epoch 52/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.8157 - val_loss: 0.4939 - val_accuracy: 0.8036\n",
      "Epoch 53/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.8191 - val_loss: 0.4692 - val_accuracy: 0.8138\n",
      "Epoch 54/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.8157 - val_loss: 0.4829 - val_accuracy: 0.7985\n",
      "Epoch 55/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.8176 - val_loss: 0.4795 - val_accuracy: 0.7985\n",
      "Epoch 56/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4519 - accuracy: 0.8159 - val_loss: 0.4674 - val_accuracy: 0.8087\n",
      "Epoch 57/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.8233 - val_loss: 0.4782 - val_accuracy: 0.8036\n",
      "Epoch 58/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4462 - accuracy: 0.8233 - val_loss: 0.4935 - val_accuracy: 0.8163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.8162 - val_loss: 0.4711 - val_accuracy: 0.8138\n",
      "Epoch 60/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.8244 - val_loss: 0.4661 - val_accuracy: 0.8138\n",
      "Epoch 61/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.8261 - val_loss: 0.4658 - val_accuracy: 0.8061\n",
      "Epoch 62/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.8242 - val_loss: 0.4671 - val_accuracy: 0.8138\n",
      "Epoch 63/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.8253 - val_loss: 0.4648 - val_accuracy: 0.8036\n",
      "Epoch 64/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4494 - accuracy: 0.8205 - val_loss: 0.4673 - val_accuracy: 0.8138\n",
      "Epoch 65/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.8259 - val_loss: 0.4663 - val_accuracy: 0.8061\n",
      "Epoch 66/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.8216 - val_loss: 0.4618 - val_accuracy: 0.8112\n",
      "Epoch 67/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4395 - accuracy: 0.8233 - val_loss: 0.4599 - val_accuracy: 0.8087\n",
      "Epoch 68/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.8216 - val_loss: 0.4644 - val_accuracy: 0.8138\n",
      "Epoch 69/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8168 - val_loss: 0.4707 - val_accuracy: 0.7908\n",
      "Epoch 70/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.8193 - val_loss: 0.4683 - val_accuracy: 0.7908\n",
      "Epoch 71/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.8233 - val_loss: 0.4811 - val_accuracy: 0.8163\n",
      "Epoch 72/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.8213 - val_loss: 0.4623 - val_accuracy: 0.8087\n",
      "Epoch 73/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.8296 - val_loss: 0.4606 - val_accuracy: 0.8112\n",
      "Epoch 74/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4338 - accuracy: 0.8222 - val_loss: 0.4612 - val_accuracy: 0.8112\n",
      "Epoch 75/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.8239 - val_loss: 0.4556 - val_accuracy: 0.8265\n",
      "Epoch 76/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8244 - val_loss: 0.4618 - val_accuracy: 0.8138\n",
      "Epoch 77/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8227 - val_loss: 0.4679 - val_accuracy: 0.8112\n",
      "Epoch 78/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8279 - val_loss: 0.4576 - val_accuracy: 0.8163\n",
      "Epoch 79/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8242 - val_loss: 0.4657 - val_accuracy: 0.7934\n",
      "Epoch 80/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8261 - val_loss: 0.4623 - val_accuracy: 0.8138\n",
      "Epoch 81/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4338 - accuracy: 0.8273 - val_loss: 0.4753 - val_accuracy: 0.7908\n",
      "Epoch 82/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.8222 - val_loss: 0.4718 - val_accuracy: 0.8189\n",
      "Epoch 83/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4270 - accuracy: 0.8276 - val_loss: 0.4617 - val_accuracy: 0.8138\n",
      "Epoch 84/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4340 - accuracy: 0.8259 - val_loss: 0.4575 - val_accuracy: 0.8010\n",
      "Epoch 85/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4293 - accuracy: 0.8216 - val_loss: 0.4508 - val_accuracy: 0.8010\n",
      "Epoch 86/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4254 - accuracy: 0.8313 - val_loss: 0.4571 - val_accuracy: 0.7934\n",
      "Epoch 87/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8250 - val_loss: 0.4557 - val_accuracy: 0.7985\n",
      "Epoch 88/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8227 - val_loss: 0.4527 - val_accuracy: 0.8061\n",
      "Epoch 89/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.8267 - val_loss: 0.4517 - val_accuracy: 0.7934\n",
      "Epoch 90/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4249 - accuracy: 0.8261 - val_loss: 0.4601 - val_accuracy: 0.8163\n",
      "Epoch 91/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4241 - accuracy: 0.8273 - val_loss: 0.4674 - val_accuracy: 0.8163\n",
      "Epoch 92/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8259 - val_loss: 0.4532 - val_accuracy: 0.8036\n",
      "Epoch 93/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4258 - accuracy: 0.8315 - val_loss: 0.4490 - val_accuracy: 0.8138\n",
      "Epoch 94/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8290 - val_loss: 0.4474 - val_accuracy: 0.7934\n",
      "Epoch 95/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4210 - accuracy: 0.8253 - val_loss: 0.4507 - val_accuracy: 0.7959\n",
      "Epoch 96/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8270 - val_loss: 0.4536 - val_accuracy: 0.7934\n",
      "Epoch 97/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4236 - accuracy: 0.8287 - val_loss: 0.4509 - val_accuracy: 0.7985\n",
      "Epoch 98/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4245 - accuracy: 0.8276 - val_loss: 0.4502 - val_accuracy: 0.8214\n",
      "Epoch 99/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8247 - val_loss: 0.4485 - val_accuracy: 0.8138\n",
      "Epoch 100/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8301 - val_loss: 0.4613 - val_accuracy: 0.8138\n",
      "Epoch 101/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4202 - accuracy: 0.8352 - val_loss: 0.4506 - val_accuracy: 0.8112\n",
      "Epoch 102/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4163 - accuracy: 0.8293 - val_loss: 0.4505 - val_accuracy: 0.7985\n",
      "Epoch 103/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8327 - val_loss: 0.4433 - val_accuracy: 0.8138\n",
      "Epoch 104/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4152 - accuracy: 0.8327 - val_loss: 0.4470 - val_accuracy: 0.8036\n",
      "Epoch 105/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4181 - accuracy: 0.8293 - val_loss: 0.4518 - val_accuracy: 0.8138\n",
      "Epoch 106/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4199 - accuracy: 0.8270 - val_loss: 0.4491 - val_accuracy: 0.8112\n",
      "Epoch 107/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4159 - accuracy: 0.8349 - val_loss: 0.4427 - val_accuracy: 0.8087\n",
      "Epoch 108/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4181 - accuracy: 0.8301 - val_loss: 0.4651 - val_accuracy: 0.7883\n",
      "Epoch 109/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4163 - accuracy: 0.8318 - val_loss: 0.4455 - val_accuracy: 0.8163\n",
      "Epoch 110/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4126 - accuracy: 0.8355 - val_loss: 0.4420 - val_accuracy: 0.8112\n",
      "Epoch 111/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4120 - accuracy: 0.8364 - val_loss: 0.4486 - val_accuracy: 0.8163\n",
      "Epoch 112/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4153 - accuracy: 0.8361 - val_loss: 0.4522 - val_accuracy: 0.8061\n",
      "Epoch 113/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8381 - val_loss: 0.4492 - val_accuracy: 0.8061\n",
      "Epoch 114/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4134 - accuracy: 0.8366 - val_loss: 0.4483 - val_accuracy: 0.8163\n",
      "Epoch 115/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4109 - accuracy: 0.8412 - val_loss: 0.4728 - val_accuracy: 0.7679\n",
      "Epoch 116/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4162 - accuracy: 0.8330 - val_loss: 0.4496 - val_accuracy: 0.8214\n",
      "Epoch 117/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4092 - accuracy: 0.8378 - val_loss: 0.4498 - val_accuracy: 0.8087\n",
      "Epoch 118/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4082 - accuracy: 0.8366 - val_loss: 0.4480 - val_accuracy: 0.8138\n",
      "Epoch 119/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4076 - accuracy: 0.8375 - val_loss: 0.4528 - val_accuracy: 0.8112\n",
      "Epoch 120/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4133 - accuracy: 0.8330 - val_loss: 0.4492 - val_accuracy: 0.8189\n",
      "Epoch 121/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4119 - accuracy: 0.8366 - val_loss: 0.4581 - val_accuracy: 0.7934\n",
      "Epoch 122/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4123 - accuracy: 0.8406 - val_loss: 0.4652 - val_accuracy: 0.7934\n",
      "Epoch 123/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4053 - accuracy: 0.8383 - val_loss: 0.4619 - val_accuracy: 0.7934\n",
      "Epoch 124/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4072 - accuracy: 0.8366 - val_loss: 0.4511 - val_accuracy: 0.8189\n",
      "Epoch 125/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4075 - accuracy: 0.8398 - val_loss: 0.4744 - val_accuracy: 0.8189\n",
      "Epoch 126/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4201 - accuracy: 0.8313 - val_loss: 0.4785 - val_accuracy: 0.8214\n",
      "Epoch 127/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4088 - accuracy: 0.8400 - val_loss: 0.4485 - val_accuracy: 0.8138\n",
      "Epoch 128/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4029 - accuracy: 0.8463 - val_loss: 0.4513 - val_accuracy: 0.8214\n",
      "Epoch 129/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.8420 - val_loss: 0.4509 - val_accuracy: 0.8036\n",
      "Epoch 130/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4029 - accuracy: 0.8471 - val_loss: 0.4721 - val_accuracy: 0.8214\n",
      "Epoch 131/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4033 - accuracy: 0.8443 - val_loss: 0.4508 - val_accuracy: 0.8138\n",
      "Epoch 132/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3992 - accuracy: 0.8454 - val_loss: 0.4563 - val_accuracy: 0.8265\n",
      "Epoch 133/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4014 - accuracy: 0.8491 - val_loss: 0.4485 - val_accuracy: 0.8163\n",
      "Epoch 134/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4033 - accuracy: 0.8469 - val_loss: 0.4519 - val_accuracy: 0.8010\n",
      "Epoch 135/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3996 - accuracy: 0.8437 - val_loss: 0.4544 - val_accuracy: 0.8036\n",
      "Epoch 136/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3994 - accuracy: 0.8449 - val_loss: 0.4525 - val_accuracy: 0.8240\n",
      "Epoch 137/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3971 - accuracy: 0.8417 - val_loss: 0.4474 - val_accuracy: 0.8189\n",
      "Epoch 138/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4084 - accuracy: 0.8505 - val_loss: 0.4853 - val_accuracy: 0.8240\n",
      "Epoch 139/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.8443 - val_loss: 0.4541 - val_accuracy: 0.8010\n",
      "Epoch 140/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3940 - accuracy: 0.8488 - val_loss: 0.4477 - val_accuracy: 0.8163\n",
      "Epoch 141/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3980 - accuracy: 0.8500 - val_loss: 0.4564 - val_accuracy: 0.8138\n",
      "Epoch 142/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8463 - val_loss: 0.4473 - val_accuracy: 0.8112\n",
      "Epoch 143/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3944 - accuracy: 0.8503 - val_loss: 0.4490 - val_accuracy: 0.8163\n",
      "Epoch 144/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3933 - accuracy: 0.8471 - val_loss: 0.4532 - val_accuracy: 0.8189\n",
      "Epoch 145/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3919 - accuracy: 0.8531 - val_loss: 0.4567 - val_accuracy: 0.8214\n",
      "Epoch 146/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3997 - accuracy: 0.8460 - val_loss: 0.4544 - val_accuracy: 0.8189\n",
      "Epoch 147/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8452 - val_loss: 0.4588 - val_accuracy: 0.8112\n",
      "Epoch 148/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4039 - accuracy: 0.8420 - val_loss: 0.4476 - val_accuracy: 0.8163\n",
      "Epoch 149/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3955 - accuracy: 0.8503 - val_loss: 0.4940 - val_accuracy: 0.7551\n",
      "Epoch 150/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4127 - accuracy: 0.8400 - val_loss: 0.4637 - val_accuracy: 0.8163\n",
      "Epoch 151/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3935 - accuracy: 0.8531 - val_loss: 0.4525 - val_accuracy: 0.8163\n",
      "Epoch 152/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8542 - val_loss: 0.4589 - val_accuracy: 0.8010\n",
      "Epoch 153/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8486 - val_loss: 0.4571 - val_accuracy: 0.8087\n",
      "Epoch 154/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3844 - accuracy: 0.8517 - val_loss: 0.4469 - val_accuracy: 0.8189\n",
      "Epoch 155/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3853 - accuracy: 0.8556 - val_loss: 0.4572 - val_accuracy: 0.8036\n",
      "Epoch 156/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8554 - val_loss: 0.4612 - val_accuracy: 0.8112\n",
      "Epoch 157/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3831 - accuracy: 0.8599 - val_loss: 0.4705 - val_accuracy: 0.8291\n",
      "Epoch 158/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3831 - accuracy: 0.8565 - val_loss: 0.4576 - val_accuracy: 0.8138\n",
      "Epoch 159/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3835 - accuracy: 0.8576 - val_loss: 0.4696 - val_accuracy: 0.8061\n",
      "Epoch 160/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3785 - accuracy: 0.8616 - val_loss: 0.4615 - val_accuracy: 0.8163\n",
      "Epoch 161/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8579 - val_loss: 0.4680 - val_accuracy: 0.7959\n",
      "Epoch 162/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3802 - accuracy: 0.8559 - val_loss: 0.4559 - val_accuracy: 0.8112\n",
      "Epoch 163/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3913 - accuracy: 0.8531 - val_loss: 0.4616 - val_accuracy: 0.8342\n",
      "Epoch 164/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8596 - val_loss: 0.4748 - val_accuracy: 0.8214\n",
      "Epoch 165/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8579 - val_loss: 0.4571 - val_accuracy: 0.8214\n",
      "Epoch 166/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3757 - accuracy: 0.8602 - val_loss: 0.4665 - val_accuracy: 0.8265\n",
      "Epoch 167/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8633 - val_loss: 0.4724 - val_accuracy: 0.8240\n",
      "Epoch 168/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3745 - accuracy: 0.8622 - val_loss: 0.4514 - val_accuracy: 0.8087\n",
      "Epoch 169/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3773 - accuracy: 0.8599 - val_loss: 0.4661 - val_accuracy: 0.7908\n",
      "Epoch 170/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.8636 - val_loss: 0.4637 - val_accuracy: 0.8087\n",
      "Epoch 171/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3711 - accuracy: 0.8656 - val_loss: 0.4636 - val_accuracy: 0.7985\n",
      "Epoch 172/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3702 - accuracy: 0.8707 - val_loss: 0.4757 - val_accuracy: 0.7883\n",
      "Epoch 173/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3659 - accuracy: 0.8698 - val_loss: 0.4600 - val_accuracy: 0.8214\n",
      "Epoch 174/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3672 - accuracy: 0.8715 - val_loss: 0.4658 - val_accuracy: 0.8112\n",
      "Epoch 175/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3658 - accuracy: 0.8712 - val_loss: 0.4725 - val_accuracy: 0.8265\n",
      "Epoch 176/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8619 - val_loss: 0.4648 - val_accuracy: 0.8214\n",
      "Epoch 177/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3642 - accuracy: 0.8690 - val_loss: 0.4565 - val_accuracy: 0.8087\n",
      "Epoch 178/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3644 - accuracy: 0.8684 - val_loss: 0.4756 - val_accuracy: 0.8138\n",
      "Epoch 179/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3768 - accuracy: 0.8639 - val_loss: 0.4664 - val_accuracy: 0.8214\n",
      "Epoch 180/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8690 - val_loss: 0.4638 - val_accuracy: 0.8061\n",
      "Epoch 181/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8727 - val_loss: 0.5117 - val_accuracy: 0.8240\n",
      "Epoch 182/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3809 - accuracy: 0.8659 - val_loss: 0.4590 - val_accuracy: 0.8163\n",
      "Epoch 183/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8667 - val_loss: 0.4734 - val_accuracy: 0.8036\n",
      "Epoch 184/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8732 - val_loss: 0.4784 - val_accuracy: 0.8189\n",
      "Epoch 185/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8707 - val_loss: 0.4720 - val_accuracy: 0.8087\n",
      "Epoch 186/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8755 - val_loss: 0.4701 - val_accuracy: 0.8138\n",
      "Epoch 187/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3584 - accuracy: 0.8746 - val_loss: 0.4807 - val_accuracy: 0.8112\n",
      "Epoch 188/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3604 - accuracy: 0.8738 - val_loss: 0.4736 - val_accuracy: 0.8138\n",
      "Epoch 189/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3575 - accuracy: 0.8749 - val_loss: 0.4824 - val_accuracy: 0.8163\n",
      "Epoch 190/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3550 - accuracy: 0.8758 - val_loss: 0.4854 - val_accuracy: 0.8138\n",
      "Epoch 191/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3619 - accuracy: 0.8769 - val_loss: 0.4700 - val_accuracy: 0.7959\n",
      "Epoch 192/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3776 - accuracy: 0.8616 - val_loss: 0.4887 - val_accuracy: 0.8010\n",
      "Epoch 193/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3607 - accuracy: 0.8744 - val_loss: 0.4877 - val_accuracy: 0.8189\n",
      "Epoch 194/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3563 - accuracy: 0.8761 - val_loss: 0.4676 - val_accuracy: 0.8061\n",
      "Epoch 195/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3506 - accuracy: 0.8843 - val_loss: 0.4697 - val_accuracy: 0.8112\n",
      "Epoch 196/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3578 - accuracy: 0.8763 - val_loss: 0.5240 - val_accuracy: 0.8163\n",
      "Epoch 197/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3534 - accuracy: 0.8815 - val_loss: 0.4711 - val_accuracy: 0.8061\n",
      "Epoch 198/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3461 - accuracy: 0.8843 - val_loss: 0.4901 - val_accuracy: 0.7934\n",
      "Epoch 199/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3557 - accuracy: 0.8783 - val_loss: 0.4873 - val_accuracy: 0.8087\n",
      "Epoch 200/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8780 - val_loss: 0.4667 - val_accuracy: 0.8087\n",
      "Epoch 201/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3477 - accuracy: 0.8817 - val_loss: 0.4646 - val_accuracy: 0.8112\n",
      "Epoch 202/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3511 - accuracy: 0.8786 - val_loss: 0.4848 - val_accuracy: 0.8036\n",
      "Epoch 203/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.8789 - val_loss: 0.4716 - val_accuracy: 0.8061\n",
      "Epoch 204/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8888 - val_loss: 0.4696 - val_accuracy: 0.7985\n",
      "Epoch 205/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3448 - accuracy: 0.8871 - val_loss: 0.4678 - val_accuracy: 0.8138\n",
      "Epoch 206/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3564 - accuracy: 0.8786 - val_loss: 0.4797 - val_accuracy: 0.8163\n",
      "Epoch 207/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3436 - accuracy: 0.8871 - val_loss: 0.4925 - val_accuracy: 0.8163\n",
      "Epoch 208/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3495 - accuracy: 0.8829 - val_loss: 0.4917 - val_accuracy: 0.8010\n",
      "Epoch 209/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8871 - val_loss: 0.4717 - val_accuracy: 0.8163\n",
      "Epoch 210/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3502 - accuracy: 0.8761 - val_loss: 0.4860 - val_accuracy: 0.8163\n",
      "Epoch 211/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8843 - val_loss: 0.4642 - val_accuracy: 0.8138\n",
      "Epoch 212/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3447 - accuracy: 0.8843 - val_loss: 0.4907 - val_accuracy: 0.8112\n",
      "Epoch 213/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3399 - accuracy: 0.8874 - val_loss: 0.4925 - val_accuracy: 0.8036\n",
      "Epoch 214/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8826 - val_loss: 0.4929 - val_accuracy: 0.7959\n",
      "Epoch 215/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8868 - val_loss: 0.4945 - val_accuracy: 0.8036\n",
      "Epoch 216/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3507 - accuracy: 0.8795 - val_loss: 0.4749 - val_accuracy: 0.8036\n",
      "Epoch 217/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3524 - accuracy: 0.8755 - val_loss: 0.4834 - val_accuracy: 0.7985\n",
      "Epoch 218/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3484 - accuracy: 0.8834 - val_loss: 0.4870 - val_accuracy: 0.8112\n",
      "Epoch 219/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8877 - val_loss: 0.4784 - val_accuracy: 0.7985\n",
      "Epoch 220/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8888 - val_loss: 0.4834 - val_accuracy: 0.8061\n",
      "Epoch 221/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8922 - val_loss: 0.4930 - val_accuracy: 0.7959\n",
      "Epoch 222/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8900 - val_loss: 0.4955 - val_accuracy: 0.8061\n",
      "Epoch 223/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8792 - val_loss: 0.4892 - val_accuracy: 0.8010\n",
      "Epoch 224/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3356 - accuracy: 0.8877 - val_loss: 0.4643 - val_accuracy: 0.8087\n",
      "Epoch 225/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3375 - accuracy: 0.8880 - val_loss: 0.4870 - val_accuracy: 0.8087\n",
      "Epoch 226/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3434 - accuracy: 0.8832 - val_loss: 0.4830 - val_accuracy: 0.8112\n",
      "Epoch 227/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3422 - accuracy: 0.8826 - val_loss: 0.4776 - val_accuracy: 0.8087\n",
      "Epoch 228/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8866 - val_loss: 0.4884 - val_accuracy: 0.7883\n",
      "Epoch 229/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8956 - val_loss: 0.4819 - val_accuracy: 0.8112\n",
      "Epoch 230/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3400 - accuracy: 0.8849 - val_loss: 0.4921 - val_accuracy: 0.7985\n",
      "Epoch 231/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8888 - val_loss: 0.4898 - val_accuracy: 0.8010\n",
      "Epoch 232/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3318 - accuracy: 0.8942 - val_loss: 0.4925 - val_accuracy: 0.8138\n",
      "Epoch 233/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8962 - val_loss: 0.4888 - val_accuracy: 0.8112\n",
      "Epoch 234/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8942 - val_loss: 0.5125 - val_accuracy: 0.8138\n",
      "Epoch 235/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3341 - accuracy: 0.8854 - val_loss: 0.5018 - val_accuracy: 0.8010\n",
      "Epoch 236/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3286 - accuracy: 0.8948 - val_loss: 0.5103 - val_accuracy: 0.8010\n",
      "Epoch 237/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3299 - accuracy: 0.8897 - val_loss: 0.4862 - val_accuracy: 0.7985\n",
      "Epoch 238/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8885 - val_loss: 0.4918 - val_accuracy: 0.8036\n",
      "Epoch 239/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3321 - accuracy: 0.8928 - val_loss: 0.4914 - val_accuracy: 0.7806\n",
      "Epoch 240/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3486 - accuracy: 0.8786 - val_loss: 0.4879 - val_accuracy: 0.8087\n",
      "Epoch 241/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3294 - accuracy: 0.8917 - val_loss: 0.5521 - val_accuracy: 0.8342\n",
      "Epoch 242/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.8877 - val_loss: 0.4803 - val_accuracy: 0.8138\n",
      "Epoch 243/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8965 - val_loss: 0.4948 - val_accuracy: 0.7985\n",
      "Epoch 244/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8968 - val_loss: 0.4847 - val_accuracy: 0.8061\n",
      "Epoch 245/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8976 - val_loss: 0.5020 - val_accuracy: 0.8138\n",
      "Epoch 246/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8959 - val_loss: 0.4998 - val_accuracy: 0.8036\n",
      "Epoch 247/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8914 - val_loss: 0.4833 - val_accuracy: 0.8112\n",
      "Epoch 248/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8917 - val_loss: 0.4911 - val_accuracy: 0.8061\n",
      "Epoch 249/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.8936 - val_loss: 0.4793 - val_accuracy: 0.8061\n",
      "Epoch 250/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8925 - val_loss: 0.4934 - val_accuracy: 0.8163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28df02520>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DNN_reg.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model_DNN_reg.fit(data_train_features_norm, keras.utils.to_categorical(train_label_values,num_classes), \n",
    "              batch_size=batch_size, epochs=epochs, \n",
    "              validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "661590c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.46744176745414734\n",
      "Test accuracy: 0.8346938490867615\n"
     ]
    }
   ],
   "source": [
    " score_reg = model_DNN_reg.evaluate(data_test_features_norm, keras.utils.to_categorical(test_label_values, num_classes), verbose=0)\n",
    "print(\"Test loss:\", score_reg[0])\n",
    "print(\"Test accuracy:\", score_reg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df09d16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "28/28 [==============================] - 1s 5ms/step - loss: 0.3398 - accuracy: 0.8979 - val_loss: 0.4981 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 2/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3195 - accuracy: 0.8945 - val_loss: 0.5244 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 3/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.9027 - val_loss: 0.5032 - val_accuracy: 0.8112 - lr: 0.0010\n",
      "Epoch 4/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3144 - accuracy: 0.8990 - val_loss: 0.4887 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 5/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3124 - accuracy: 0.9010 - val_loss: 0.5315 - val_accuracy: 0.8112 - lr: 0.0010\n",
      "Epoch 6/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3119 - accuracy: 0.9022 - val_loss: 0.5195 - val_accuracy: 0.8087 - lr: 0.0010\n",
      "Epoch 7/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3120 - accuracy: 0.8993 - val_loss: 0.5097 - val_accuracy: 0.8189 - lr: 0.0010\n",
      "Epoch 8/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8993 - val_loss: 0.5015 - val_accuracy: 0.8189 - lr: 0.0010\n",
      "Epoch 9/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3047 - accuracy: 0.9022 - val_loss: 0.5132 - val_accuracy: 0.8112 - lr: 0.0010\n",
      "Epoch 10/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3105 - accuracy: 0.9007 - val_loss: 0.4966 - val_accuracy: 0.7959 - lr: 0.0010\n",
      "Epoch 11/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.9013 - val_loss: 0.5675 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 12/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8885 - val_loss: 0.5316 - val_accuracy: 0.7959 - lr: 0.0010\n",
      "Epoch 13/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8951 - val_loss: 0.5186 - val_accuracy: 0.8010 - lr: 0.0010\n",
      "Epoch 14/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3175 - accuracy: 0.8917 - val_loss: 0.5143 - val_accuracy: 0.8087 - lr: 0.0010\n",
      "Epoch 15/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.9024 - val_loss: 0.5750 - val_accuracy: 0.8265 - lr: 0.0010\n",
      "Epoch 16/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8979 - val_loss: 0.5266 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 17/250\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3169 - accuracy: 0.8973 - val_loss: 0.5162 - val_accuracy: 0.8265 - lr: 0.0010\n",
      "Epoch 18/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3124 - accuracy: 0.8999 - val_loss: 0.5057 - val_accuracy: 0.8087 - lr: 0.0010\n",
      "Epoch 19/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8985 - val_loss: 0.5217 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 20/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3099 - accuracy: 0.9007 - val_loss: 0.5267 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 21/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3057 - accuracy: 0.9050 - val_loss: 0.5150 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 22/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.3043 - accuracy: 0.9098 - val_loss: 0.5191 - val_accuracy: 0.8010 - lr: 0.0010\n",
      "Epoch 23/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3019 - accuracy: 0.9019 - val_loss: 0.5276 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 24/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3021 - accuracy: 0.9070 - val_loss: 0.5421 - val_accuracy: 0.8010 - lr: 0.0010\n",
      "Epoch 25/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3052 - accuracy: 0.9010 - val_loss: 0.5457 - val_accuracy: 0.8087 - lr: 0.0010\n",
      "Epoch 26/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2952 - accuracy: 0.9061 - val_loss: 0.5096 - val_accuracy: 0.8112 - lr: 1.0000e-04\n",
      "Epoch 27/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.9144 - val_loss: 0.5080 - val_accuracy: 0.8138 - lr: 1.0000e-04\n",
      "Epoch 28/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.9161 - val_loss: 0.5078 - val_accuracy: 0.8163 - lr: 1.0000e-04\n",
      "Epoch 29/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.9186 - val_loss: 0.5074 - val_accuracy: 0.8189 - lr: 1.0000e-04\n",
      "Epoch 30/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2661 - accuracy: 0.9183 - val_loss: 0.5139 - val_accuracy: 0.8189 - lr: 1.0000e-04\n",
      "Epoch 31/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.9180 - val_loss: 0.5117 - val_accuracy: 0.8138 - lr: 1.0000e-04\n",
      "Epoch 32/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.9178 - val_loss: 0.5116 - val_accuracy: 0.8163 - lr: 1.0000e-04\n",
      "Epoch 33/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2655 - accuracy: 0.9192 - val_loss: 0.5124 - val_accuracy: 0.8138 - lr: 1.0000e-04\n",
      "Epoch 34/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.9189 - val_loss: 0.5152 - val_accuracy: 0.8138 - lr: 1.0000e-04\n",
      "Epoch 35/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.9175 - val_loss: 0.5192 - val_accuracy: 0.8112 - lr: 1.0000e-04\n",
      "Epoch 36/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.9186 - val_loss: 0.5179 - val_accuracy: 0.8112 - lr: 1.0000e-05\n",
      "Epoch 37/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.9183 - val_loss: 0.5175 - val_accuracy: 0.8112 - lr: 1.0000e-05\n",
      "Epoch 38/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2618 - accuracy: 0.9183 - val_loss: 0.5172 - val_accuracy: 0.8112 - lr: 1.0000e-05\n",
      "Epoch 39/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.9183 - val_loss: 0.5174 - val_accuracy: 0.8112 - lr: 1.0000e-05\n",
      "Epoch 40/250\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.9189 - val_loss: 0.5170 - val_accuracy: 0.8138 - lr: 1.0000e-05\n",
      "Epoch 41/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.9189 - val_loss: 0.5167 - val_accuracy: 0.8138 - lr: 1.0000e-05\n",
      "Epoch 42/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2616 - accuracy: 0.9183 - val_loss: 0.5176 - val_accuracy: 0.8112 - lr: 1.0000e-05\n",
      "Epoch 43/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.9186 - val_loss: 0.5176 - val_accuracy: 0.8112 - lr: 1.0000e-05\n",
      "Epoch 44/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2616 - accuracy: 0.9186 - val_loss: 0.5169 - val_accuracy: 0.8138 - lr: 1.0000e-05\n",
      "Epoch 45/250\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2615 - accuracy: 0.9186 - val_loss: 0.5175 - val_accuracy: 0.8112 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "#Add callback and lr\n",
    "model_DNN_reg.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, \n",
    "                                      verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "lr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10, \n",
    "                                       verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "history=model_DNN_reg.fit(data_train_features_norm.reshape(3918,11,1), \n",
    "              keras.utils.to_categorical(train_label_values,num_classes), \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_split=0.1,\n",
    "              callbacks=[early,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c9163fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.47235870361328125\n",
      "Test accuracy: 0.8377550840377808\n"
     ]
    }
   ],
   "source": [
    "score_reg_new = model_DNN_reg.evaluate(data_test_features_norm, keras.utils.to_categorical(test_label_values, num_classes), verbose=0)\n",
    "print(\"Test loss:\", score_reg_new[0])\n",
    "print(\"Test accuracy:\", score_reg_new[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f66cff9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting accuracy values for training & validation\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "22f08cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing learning rates\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_DNN_reg.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "48c6e76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 2/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 3/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 4/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 5/300\n",
      "56/56 [==============================] - 0s 998us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 6/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 7/300\n",
      "56/56 [==============================] - 0s 988us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 8/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 9/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 10/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 11/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-27\n",
      "Epoch 12/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 13/300\n",
      "56/56 [==============================] - 0s 993us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 14/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 15/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 16/300\n",
      "56/56 [==============================] - 0s 997us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 17/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 18/300\n",
      "56/56 [==============================] - 0s 1000us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 19/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 20/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 21/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-28\n",
      "Epoch 22/300\n",
      "56/56 [==============================] - 0s 999us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 23/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 24/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 25/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 26/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 27/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 28/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 29/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 30/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 31/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-29\n",
      "Epoch 32/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 33/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 34/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 35/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 36/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 37/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 38/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 39/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 40/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 41/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-30\n",
      "Epoch 42/300\n",
      "56/56 [==============================] - 0s 993us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 43/300\n",
      "56/56 [==============================] - 0s 992us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 44/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 45/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 46/300\n",
      "56/56 [==============================] - 0s 989us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 47/300\n",
      "56/56 [==============================] - 0s 987us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 48/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 49/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 50/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 51/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-31\n",
      "Epoch 52/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 53/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 54/300\n",
      "56/56 [==============================] - 0s 972us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 55/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 56/300\n",
      "56/56 [==============================] - 0s 988us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 57/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 58/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 59/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 60/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 61/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-32\n",
      "Epoch 62/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 63/300\n",
      "56/56 [==============================] - 0s 986us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 64/300\n",
      "56/56 [==============================] - 0s 989us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 65/300\n",
      "56/56 [==============================] - 0s 999us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 66/300\n",
      "56/56 [==============================] - 0s 985us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 67/300\n",
      "56/56 [==============================] - 0s 988us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 68/300\n",
      "56/56 [==============================] - 0s 965us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 69/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 70/300\n",
      "56/56 [==============================] - 0s 998us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 71/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-33\n",
      "Epoch 72/300\n",
      "56/56 [==============================] - 0s 997us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 73/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 74/300\n",
      "56/56 [==============================] - 0s 990us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 75/300\n",
      "56/56 [==============================] - 0s 983us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 76/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 77/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 78/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 79/300\n",
      "56/56 [==============================] - 0s 988us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 80/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 81/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-34\n",
      "Epoch 82/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 83/300\n",
      "56/56 [==============================] - 0s 985us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 84/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 85/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 86/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 87/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 88/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 89/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 90/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 91/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-35\n",
      "Epoch 92/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 93/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 94/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 95/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 96/300\n",
      "56/56 [==============================] - 0s 1000us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 97/300\n",
      "56/56 [==============================] - 0s 980us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 98/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 99/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 100/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 101/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-36\n",
      "Epoch 102/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 103/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 104/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 105/300\n",
      "56/56 [==============================] - 0s 982us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 106/300\n",
      "56/56 [==============================] - 0s 977us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 107/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 108/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 109/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 110/300\n",
      "56/56 [==============================] - 0s 990us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 111/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-37\n",
      "Epoch 112/300\n",
      "56/56 [==============================] - 0s 977us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 113/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 114/300\n",
      "56/56 [==============================] - 0s 998us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 115/300\n",
      "56/56 [==============================] - 0s 989us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 116/300\n",
      "56/56 [==============================] - 0s 976us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 117/300\n",
      "56/56 [==============================] - 0s 985us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 118/300\n",
      "56/56 [==============================] - 0s 992us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 119/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 120/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 121/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-38\n",
      "Epoch 122/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 123/300\n",
      "56/56 [==============================] - 0s 979us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 124/300\n",
      "56/56 [==============================] - 0s 992us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 125/300\n",
      "56/56 [==============================] - 0s 983us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 126/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 127/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 128/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 129/300\n",
      "56/56 [==============================] - 0s 993us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 130/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 131/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0000e-39\n",
      "Epoch 132/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 133/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 134/300\n",
      "56/56 [==============================] - 0s 987us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 135/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 136/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 137/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 138/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 139/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 140/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 141/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9999e-41\n",
      "Epoch 142/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 143/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 144/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 145/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 146/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 147/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 148/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 149/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 150/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 151/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9997e-42\n",
      "Epoch 152/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 153/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 155/300\n",
      "56/56 [==============================] - 0s 993us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 156/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 157/300\n",
      "56/56 [==============================] - 0s 986us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 158/300\n",
      "56/56 [==============================] - 0s 973us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 159/300\n",
      "56/56 [==============================] - 0s 979us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 160/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 161/300\n",
      "56/56 [==============================] - 0s 976us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.0005e-42\n",
      "Epoch 162/300\n",
      "56/56 [==============================] - 0s 986us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 163/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 164/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 165/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 166/300\n",
      "56/56 [==============================] - 0s 979us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 167/300\n",
      "56/56 [==============================] - 0s 977us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 168/300\n",
      "56/56 [==============================] - 0s 988us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 169/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 170/300\n",
      "56/56 [==============================] - 0s 990us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 171/300\n",
      "56/56 [==============================] - 0s 984us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.9492e-44\n",
      "Epoch 172/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 173/300\n",
      "56/56 [==============================] - 0s 993us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 174/300\n",
      "56/56 [==============================] - 0s 985us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 175/300\n",
      "56/56 [==============================] - 0s 987us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 176/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 177/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 178/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 179/300\n",
      "56/56 [==============================] - 0s 987us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 180/300\n",
      "56/56 [==============================] - 0s 977us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 181/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 9.8091e-45\n",
      "Epoch 182/300\n",
      "56/56 [==============================] - 0s 975us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 183/300\n",
      "56/56 [==============================] - 0s 988us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 184/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 185/300\n",
      "56/56 [==============================] - 0s 978us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 186/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 187/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 188/300\n",
      "56/56 [==============================] - 0s 989us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 189/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 190/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 191/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 1.4013e-45\n",
      "Epoch 192/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 193/300\n",
      "56/56 [==============================] - 0s 992us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 194/300\n",
      "56/56 [==============================] - 0s 984us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 195/300\n",
      "56/56 [==============================] - 0s 990us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 196/300\n",
      "56/56 [==============================] - 0s 992us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 197/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 198/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 199/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 200/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 201/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 202/300\n",
      "56/56 [==============================] - 0s 984us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 203/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 204/300\n",
      "56/56 [==============================] - 0s 997us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 206/300\n",
      "56/56 [==============================] - 0s 985us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 207/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 208/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 209/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 210/300\n",
      "56/56 [==============================] - 0s 987us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 211/300\n",
      "56/56 [==============================] - 0s 983us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 212/300\n",
      "56/56 [==============================] - 0s 987us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 213/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 214/300\n",
      "56/56 [==============================] - 0s 984us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 215/300\n",
      "56/56 [==============================] - 0s 982us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 216/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 217/300\n",
      "56/56 [==============================] - 0s 999us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 218/300\n",
      "56/56 [==============================] - 0s 987us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 219/300\n",
      "56/56 [==============================] - 0s 992us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 220/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 221/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 222/300\n",
      "56/56 [==============================] - 0s 993us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 223/300\n",
      "56/56 [==============================] - 0s 993us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 224/300\n",
      "56/56 [==============================] - 0s 988us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 225/300\n",
      "56/56 [==============================] - 0s 987us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 226/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 227/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 228/300\n",
      "56/56 [==============================] - 0s 993us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 229/300\n",
      "56/56 [==============================] - 0s 989us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 230/300\n",
      "56/56 [==============================] - 0s 993us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 231/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 232/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 233/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 234/300\n",
      "56/56 [==============================] - 0s 989us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 235/300\n",
      "56/56 [==============================] - 0s 992us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 236/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 237/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 238/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 239/300\n",
      "56/56 [==============================] - 0s 986us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 240/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 241/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 242/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 243/300\n",
      "56/56 [==============================] - 0s 999us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 244/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 245/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 246/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 247/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 248/300\n",
      "56/56 [==============================] - 0s 986us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 249/300\n",
      "56/56 [==============================] - 0s 998us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 250/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 251/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 252/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 253/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 254/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 255/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 257/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 258/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 259/300\n",
      "56/56 [==============================] - 0s 983us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 260/300\n",
      "56/56 [==============================] - 0s 984us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 261/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 262/300\n",
      "56/56 [==============================] - 0s 972us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 263/300\n",
      "56/56 [==============================] - 0s 980us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 264/300\n",
      "56/56 [==============================] - 0s 981us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 265/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 266/300\n",
      "56/56 [==============================] - 0s 986us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 267/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 268/300\n",
      "56/56 [==============================] - 0s 987us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 269/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 270/300\n",
      "56/56 [==============================] - 0s 989us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 271/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 272/300\n",
      "56/56 [==============================] - 0s 983us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 273/300\n",
      "56/56 [==============================] - 0s 988us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 274/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 275/300\n",
      "56/56 [==============================] - 0s 990us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 276/300\n",
      "56/56 [==============================] - 0s 989us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 277/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 278/300\n",
      "56/56 [==============================] - 0s 990us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 279/300\n",
      "56/56 [==============================] - 0s 997us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 280/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 281/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 282/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 283/300\n",
      "56/56 [==============================] - 0s 996us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 284/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 285/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 286/300\n",
      "56/56 [==============================] - 0s 995us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 287/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 288/300\n",
      "56/56 [==============================] - 0s 977us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 289/300\n",
      "56/56 [==============================] - 0s 994us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 290/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 291/300\n",
      "56/56 [==============================] - 0s 999us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 292/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 293/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 294/300\n",
      "56/56 [==============================] - 0s 991us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 295/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 296/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 297/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 298/300\n",
      "56/56 [==============================] - 0s 998us/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 299/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n",
      "Epoch 300/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9223 - val_loss: 0.5369 - val_accuracy: 0.8112 - lr: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10)\n",
    "history_new=model_DNN_reg.fit(data_train_features_norm, keras.utils.to_categorical(train_label_values,num_classes), \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_split=0.1,\n",
    "                  callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dab84473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4792219400405884\n",
      "Test accuracy: 0.8408163189888\n"
     ]
    }
   ],
   "source": [
    "score = model_DNN_reg.evaluate(data_test_features_norm, keras.utils.to_categorical(test_label_values, num_classes), verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "m1CaFMVmea00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m1CaFMVmea00",
    "outputId": "c6a52f41-aa29-4800-d746-97930d40d406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 9, 16)             64        \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 4, 16)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 2, 32)             1568      \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,762\n",
      "Trainable params: 1,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ConvolutionalNetwork\n",
    "model_CNN = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(11,1)),\n",
    "        layers.Conv1D(16, kernel_size=3, activation=\"relu\"), # 16 different 3x3 kernels\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Conv1D(32, kernel_size=3, activation=\"relu\"), # 32 different 3x3 kernels\n",
    "        layers.Flatten(),               # reshape multi-dimensional array into a vector.\n",
    "        layers.Dropout(0.5),            # randomly select 50% the neurons to dropout \n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "Ed6e2uYTea3L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ed6e2uYTea3L",
    "outputId": "b49633cf-fa0f-42ff-82bb-3bdfdab9faf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.5665 - accuracy: 0.7187 - val_loss: 0.4749 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "56/56 [==============================] - 0s 788us/step - loss: 0.5046 - accuracy: 0.7805 - val_loss: 0.4654 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "56/56 [==============================] - 0s 750us/step - loss: 0.4899 - accuracy: 0.7805 - val_loss: 0.4592 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "56/56 [==============================] - 0s 758us/step - loss: 0.4818 - accuracy: 0.7816 - val_loss: 0.4530 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "56/56 [==============================] - 0s 767us/step - loss: 0.4737 - accuracy: 0.7822 - val_loss: 0.4478 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "56/56 [==============================] - 0s 755us/step - loss: 0.4657 - accuracy: 0.7842 - val_loss: 0.4454 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "56/56 [==============================] - 0s 757us/step - loss: 0.4582 - accuracy: 0.7901 - val_loss: 0.4417 - val_accuracy: 0.7883 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "56/56 [==============================] - 0s 763us/step - loss: 0.4533 - accuracy: 0.7924 - val_loss: 0.4349 - val_accuracy: 0.7959 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "56/56 [==============================] - 0s 790us/step - loss: 0.4519 - accuracy: 0.7867 - val_loss: 0.4360 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "56/56 [==============================] - 0s 781us/step - loss: 0.4459 - accuracy: 0.7915 - val_loss: 0.4312 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "56/56 [==============================] - 0s 776us/step - loss: 0.4455 - accuracy: 0.7890 - val_loss: 0.4268 - val_accuracy: 0.7959 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "56/56 [==============================] - 0s 790us/step - loss: 0.4409 - accuracy: 0.7950 - val_loss: 0.4233 - val_accuracy: 0.7985 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "56/56 [==============================] - 0s 827us/step - loss: 0.4392 - accuracy: 0.7876 - val_loss: 0.4190 - val_accuracy: 0.8010 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "56/56 [==============================] - 0s 769us/step - loss: 0.4295 - accuracy: 0.7933 - val_loss: 0.4248 - val_accuracy: 0.7934 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "56/56 [==============================] - 0s 763us/step - loss: 0.4285 - accuracy: 0.7967 - val_loss: 0.4156 - val_accuracy: 0.7883 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "56/56 [==============================] - 0s 754us/step - loss: 0.4260 - accuracy: 0.7958 - val_loss: 0.4142 - val_accuracy: 0.7985 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "56/56 [==============================] - 0s 792us/step - loss: 0.4244 - accuracy: 0.8001 - val_loss: 0.4137 - val_accuracy: 0.8010 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "56/56 [==============================] - 0s 807us/step - loss: 0.4202 - accuracy: 0.8037 - val_loss: 0.4099 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "56/56 [==============================] - 0s 779us/step - loss: 0.4149 - accuracy: 0.8094 - val_loss: 0.4081 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "56/56 [==============================] - 0s 806us/step - loss: 0.4146 - accuracy: 0.8063 - val_loss: 0.4065 - val_accuracy: 0.8010 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "56/56 [==============================] - 0s 777us/step - loss: 0.4173 - accuracy: 0.7998 - val_loss: 0.4064 - val_accuracy: 0.8087 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "56/56 [==============================] - 0s 763us/step - loss: 0.4110 - accuracy: 0.8035 - val_loss: 0.4037 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "56/56 [==============================] - 0s 796us/step - loss: 0.4168 - accuracy: 0.8023 - val_loss: 0.4036 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "56/56 [==============================] - 0s 774us/step - loss: 0.4127 - accuracy: 0.8057 - val_loss: 0.4063 - val_accuracy: 0.8087 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "56/56 [==============================] - 0s 780us/step - loss: 0.4134 - accuracy: 0.8069 - val_loss: 0.4038 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "56/56 [==============================] - 0s 804us/step - loss: 0.4129 - accuracy: 0.8049 - val_loss: 0.4040 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "56/56 [==============================] - 0s 768us/step - loss: 0.4079 - accuracy: 0.8040 - val_loss: 0.4004 - val_accuracy: 0.8112 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "56/56 [==============================] - 0s 769us/step - loss: 0.4037 - accuracy: 0.8080 - val_loss: 0.3996 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "56/56 [==============================] - 0s 878us/step - loss: 0.4061 - accuracy: 0.8071 - val_loss: 0.4020 - val_accuracy: 0.8112 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "56/56 [==============================] - 0s 888us/step - loss: 0.4075 - accuracy: 0.8057 - val_loss: 0.3969 - val_accuracy: 0.8189 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.3984 - accuracy: 0.8083 - val_loss: 0.3986 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "56/56 [==============================] - 0s 775us/step - loss: 0.4028 - accuracy: 0.8023 - val_loss: 0.3957 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "56/56 [==============================] - 0s 778us/step - loss: 0.4052 - accuracy: 0.8066 - val_loss: 0.3968 - val_accuracy: 0.8087 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "56/56 [==============================] - 0s 773us/step - loss: 0.3942 - accuracy: 0.8148 - val_loss: 0.3966 - val_accuracy: 0.8240 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "56/56 [==============================] - 0s 798us/step - loss: 0.3957 - accuracy: 0.8108 - val_loss: 0.3957 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "56/56 [==============================] - 0s 983us/step - loss: 0.4051 - accuracy: 0.8083 - val_loss: 0.3940 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "56/56 [==============================] - 0s 799us/step - loss: 0.3994 - accuracy: 0.8094 - val_loss: 0.3976 - val_accuracy: 0.8138 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "56/56 [==============================] - 0s 797us/step - loss: 0.3936 - accuracy: 0.8134 - val_loss: 0.3922 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "56/56 [==============================] - 0s 759us/step - loss: 0.4004 - accuracy: 0.8106 - val_loss: 0.3939 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "56/56 [==============================] - 0s 775us/step - loss: 0.3986 - accuracy: 0.8088 - val_loss: 0.3922 - val_accuracy: 0.8240 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "56/56 [==============================] - 0s 760us/step - loss: 0.3898 - accuracy: 0.8057 - val_loss: 0.3924 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "56/56 [==============================] - 0s 759us/step - loss: 0.3914 - accuracy: 0.8063 - val_loss: 0.3921 - val_accuracy: 0.8291 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "56/56 [==============================] - 0s 770us/step - loss: 0.3942 - accuracy: 0.8125 - val_loss: 0.3925 - val_accuracy: 0.8189 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "56/56 [==============================] - 0s 763us/step - loss: 0.3917 - accuracy: 0.8131 - val_loss: 0.3880 - val_accuracy: 0.8265 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "56/56 [==============================] - 0s 791us/step - loss: 0.3977 - accuracy: 0.8120 - val_loss: 0.3918 - val_accuracy: 0.8189 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "56/56 [==============================] - 0s 777us/step - loss: 0.3931 - accuracy: 0.8080 - val_loss: 0.3867 - val_accuracy: 0.8240 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "56/56 [==============================] - 0s 765us/step - loss: 0.3866 - accuracy: 0.8210 - val_loss: 0.3896 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "56/56 [==============================] - 0s 793us/step - loss: 0.3904 - accuracy: 0.8134 - val_loss: 0.3893 - val_accuracy: 0.8087 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "56/56 [==============================] - 0s 777us/step - loss: 0.3945 - accuracy: 0.8100 - val_loss: 0.3867 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "56/56 [==============================] - 0s 765us/step - loss: 0.3872 - accuracy: 0.8137 - val_loss: 0.3868 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "56/56 [==============================] - 0s 820us/step - loss: 0.3928 - accuracy: 0.8052 - val_loss: 0.3878 - val_accuracy: 0.8265 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "56/56 [==============================] - 0s 793us/step - loss: 0.3891 - accuracy: 0.8111 - val_loss: 0.3885 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "56/56 [==============================] - 0s 768us/step - loss: 0.3829 - accuracy: 0.8188 - val_loss: 0.3875 - val_accuracy: 0.8087 - lr: 1.0000e-04\n",
      "Epoch 54/300\n",
      "56/56 [==============================] - 0s 766us/step - loss: 0.3919 - accuracy: 0.8103 - val_loss: 0.3874 - val_accuracy: 0.8138 - lr: 1.0000e-04\n",
      "Epoch 55/300\n",
      "56/56 [==============================] - 0s 754us/step - loss: 0.3866 - accuracy: 0.8176 - val_loss: 0.3875 - val_accuracy: 0.8163 - lr: 1.0000e-04\n",
      "Epoch 56/300\n",
      "56/56 [==============================] - 0s 804us/step - loss: 0.3839 - accuracy: 0.8111 - val_loss: 0.3873 - val_accuracy: 0.8138 - lr: 1.0000e-04\n",
      "Epoch 57/300\n",
      "56/56 [==============================] - 0s 785us/step - loss: 0.3821 - accuracy: 0.8128 - val_loss: 0.3870 - val_accuracy: 0.8163 - lr: 1.0000e-04\n",
      "Epoch 58/300\n",
      "56/56 [==============================] - 0s 759us/step - loss: 0.3855 - accuracy: 0.8142 - val_loss: 0.3866 - val_accuracy: 0.8189 - lr: 1.0000e-04\n",
      "Epoch 59/300\n",
      "56/56 [==============================] - 0s 746us/step - loss: 0.3848 - accuracy: 0.8154 - val_loss: 0.3864 - val_accuracy: 0.8214 - lr: 1.0000e-04\n",
      "Epoch 60/300\n",
      "56/56 [==============================] - 0s 755us/step - loss: 0.3818 - accuracy: 0.8182 - val_loss: 0.3862 - val_accuracy: 0.8189 - lr: 1.0000e-04\n",
      "Epoch 61/300\n",
      "56/56 [==============================] - 0s 774us/step - loss: 0.3820 - accuracy: 0.8176 - val_loss: 0.3862 - val_accuracy: 0.8214 - lr: 1.0000e-04\n",
      "Epoch 62/300\n",
      "56/56 [==============================] - 0s 744us/step - loss: 0.3873 - accuracy: 0.8157 - val_loss: 0.3858 - val_accuracy: 0.8240 - lr: 1.0000e-04\n",
      "Epoch 63/300\n",
      "56/56 [==============================] - 0s 756us/step - loss: 0.3852 - accuracy: 0.8162 - val_loss: 0.3859 - val_accuracy: 0.8240 - lr: 1.0000e-05\n",
      "Epoch 64/300\n",
      "56/56 [==============================] - 0s 755us/step - loss: 0.3812 - accuracy: 0.8205 - val_loss: 0.3859 - val_accuracy: 0.8240 - lr: 1.0000e-05\n",
      "Epoch 65/300\n",
      "56/56 [==============================] - 0s 749us/step - loss: 0.3780 - accuracy: 0.8168 - val_loss: 0.3859 - val_accuracy: 0.8240 - lr: 1.0000e-05\n",
      "Epoch 66/300\n",
      "56/56 [==============================] - 0s 753us/step - loss: 0.3830 - accuracy: 0.8154 - val_loss: 0.3860 - val_accuracy: 0.8214 - lr: 1.0000e-05\n",
      "Epoch 67/300\n",
      "56/56 [==============================] - 0s 733us/step - loss: 0.3895 - accuracy: 0.8083 - val_loss: 0.3859 - val_accuracy: 0.8240 - lr: 1.0000e-05\n",
      "Epoch 68/300\n",
      "56/56 [==============================] - 0s 767us/step - loss: 0.3792 - accuracy: 0.8179 - val_loss: 0.3860 - val_accuracy: 0.8240 - lr: 1.0000e-05\n",
      "Epoch 69/300\n",
      "56/56 [==============================] - 0s 762us/step - loss: 0.3813 - accuracy: 0.8202 - val_loss: 0.3860 - val_accuracy: 0.8240 - lr: 1.0000e-05\n",
      "Epoch 70/300\n",
      "56/56 [==============================] - 0s 768us/step - loss: 0.3887 - accuracy: 0.8091 - val_loss: 0.3860 - val_accuracy: 0.8240 - lr: 1.0000e-05\n",
      "Epoch 71/300\n",
      "56/56 [==============================] - 0s 770us/step - loss: 0.3818 - accuracy: 0.8159 - val_loss: 0.3860 - val_accuracy: 0.8240 - lr: 1.0000e-05\n",
      "Epoch 72/300\n",
      "56/56 [==============================] - 0s 745us/step - loss: 0.3801 - accuracy: 0.8219 - val_loss: 0.3859 - val_accuracy: 0.8240 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29314f250>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 300\n",
    "\n",
    "model_CNN.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=30, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "lr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "model_CNN.fit(data_train_features_norm.reshape(3918,11,1), keras.utils.to_categorical(train_label_values,num_classes), batch_size=batch_size, epochs=epochs, validation_split=0.1,callbacks=[early,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c4d85d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(980, 11)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_features_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "m_rz-RWKea5v",
   "metadata": {
    "id": "m_rz-RWKea5v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 555us/step - loss: 0.3505 - accuracy: 0.8418\n",
      "Test loss: 0.35045021772384644\n",
      "Test accuracy: 0.8418367505073547\n"
     ]
    }
   ],
   "source": [
    "score = model_CNN.evaluate(data_test_features_norm.reshape(980,11,1), keras.utils.to_categorical(test_label_values,num_classes))\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
